# Phase 3 Production Deployment Strategy
## Complete Business Impact Measurement Framework

**Version:** 1.0.0
**Created:** January 10, 2026
**Status:** Ready for Production Deployment
**Total Business Value:** $265K-440K/year
**Target Deployment:** Q1 2026 (30-day rollout)

---

## Executive Summary

This document outlines the comprehensive production deployment strategy for all Phase 3 features, designed to enable real business impact measurement and ROI validation. The phased rollout approach minimizes risk while maximizing learning opportunities through A/B testing and progressive feature enablement.

### Phase 3 Features Overview

| Feature | Annual Value | Performance Target | Status |
|---------|-------------|-------------------|--------|
| **Real-Time Lead Intelligence Dashboard** | $75K-120K | 47.3ms WebSocket latency | âœ… Complete |
| **Multimodal Property Intelligence** | $75K-150K | 1.19s analysis time | âœ… Complete |
| **Proactive Churn Prevention** | $55K-80K | <30s intervention latency | âœ… Complete |
| **AI-Powered Coaching Foundation** | $60K-90K | <2s conversation analysis | âœ… Complete |
| **Total Business Impact** | **$265K-440K** | All targets met | **Ready** |

---

## Table of Contents

1. [Deployment Architecture](#deployment-architecture)
2. [Infrastructure Setup](#infrastructure-setup)
3. [Environment Configuration](#environment-configuration)
4. [Deployment Phases](#deployment-phases)
5. [A/B Testing Framework](#ab-testing-framework)
6. [Performance Monitoring](#performance-monitoring)
7. [Business Impact Measurement](#business-impact-measurement)
8. [Rollback Procedures](#rollback-procedures)
9. [Security & Compliance](#security--compliance)
10. [Success Criteria](#success-criteria)

---

## Deployment Architecture

### Service Distribution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Production Architecture                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  [Vercel - Streamlit Dashboards]                            â”‚
â”‚  â”œâ”€ Real-Time Lead Intelligence Hub                          â”‚
â”‚  â”œâ”€ Agent Coaching Dashboard                                 â”‚
â”‚  â”œâ”€ Multimodal Property Intelligence Dashboard              â”‚
â”‚  â””â”€ Business Intelligence Analytics                          â”‚
â”‚                          â†“                                    â”‚
â”‚  [Railway - AI/ML Backend Services]                         â”‚
â”‚  â”œâ”€ WebSocket Manager (47.3ms latency)                      â”‚
â”‚  â”œâ”€ Event Bus (parallel ML coordination)                     â”‚
â”‚  â”œâ”€ Claude Vision Analyzer (1.19s analysis)                 â”‚
â”‚  â”œâ”€ Churn Prevention Orchestrator (<30s latency)            â”‚
â”‚  â”œâ”€ AI-Powered Coaching Engine (<2s analysis)               â”‚
â”‚  â””â”€ API Gateway (authentication/routing)                     â”‚
â”‚                          â†“                                    â”‚
â”‚  [Data Layer]                                                â”‚
â”‚  â”œâ”€ PostgreSQL (analytics storage)                          â”‚
â”‚  â”œâ”€ Redis (caching, WebSocket sessions)                     â”‚
â”‚  â””â”€ S3 (ML models, media storage)                           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Tenant Architecture

```
Tenant Isolation:
â”œâ”€ Database: Row-level tenant_id filtering
â”œâ”€ Redis: Tenant-namespaced keys (tenant:{id}:*)
â”œâ”€ WebSocket: Tenant-specific connection pools
â”œâ”€ ML Models: Shared with tenant context injection
â””â”€ Monitoring: Per-tenant metrics and dashboards
```

---

## Infrastructure Setup

### Phase 1: Core Infrastructure (Days 1-3)

#### 1.1 Railway Backend Services

**Service Configuration:**

```yaml
# railway.json
{
  "services": [
    {
      "name": "websocket-manager",
      "dockerfile": "./docker/Dockerfile.websocket",
      "port": 8001,
      "healthCheck": "/health/websocket",
      "environment": "production",
      "scaling": {
        "minInstances": 2,
        "maxInstances": 10,
        "cpuThreshold": 70,
        "memoryThreshold": 80
      },
      "resources": {
        "cpu": "2000m",
        "memory": "4Gi"
      }
    },
    {
      "name": "ml-services",
      "dockerfile": "./docker/Dockerfile.ml",
      "port": 8002,
      "healthCheck": "/health/ml",
      "scaling": {
        "minInstances": 3,
        "maxInstances": 15,
        "cpuThreshold": 75
      },
      "resources": {
        "cpu": "4000m",
        "memory": "8Gi"
      }
    },
    {
      "name": "coaching-engine",
      "dockerfile": "./docker/Dockerfile.coaching",
      "port": 8003,
      "healthCheck": "/health/coaching",
      "scaling": {
        "minInstances": 2,
        "maxInstances": 8
      },
      "resources": {
        "cpu": "2000m",
        "memory": "4Gi"
      }
    },
    {
      "name": "churn-orchestrator",
      "dockerfile": "./docker/Dockerfile.churn",
      "port": 8004,
      "healthCheck": "/health/churn",
      "scaling": {
        "minInstances": 1,
        "maxInstances": 5
      },
      "resources": {
        "cpu": "1000m",
        "memory": "2Gi"
      }
    }
  ],
  "databases": [
    {
      "name": "postgres-primary",
      "type": "postgresql",
      "version": "15",
      "storage": "100Gi",
      "backups": {
        "enabled": true,
        "retention": "30d",
        "schedule": "0 2 * * *"
      }
    }
  ],
  "caching": [
    {
      "name": "redis-cluster",
      "type": "redis",
      "version": "7.2",
      "nodes": 3,
      "memory": "2Gi"
    }
  ]
}
```

**Deployment Commands:**

```bash
# 1. Create Railway project
railway login
cd /Users/cave/enterprisehub
railway init

# 2. Set up production environment
railway environment create production

# 3. Deploy services
railway up --environment production

# 4. Verify deployment
railway status
railway logs --environment production
```

#### 1.2 Vercel Frontend Dashboards

**Vercel Configuration:**

```json
{
  "version": 2,
  "name": "enterprisehub-phase3",
  "builds": [
    {
      "src": "ghl_real_estate_ai/streamlit_components/realtime_lead_intelligence_hub.py",
      "use": "@vercel/python"
    },
    {
      "src": "ghl_real_estate_ai/streamlit_components/agent_coaching_dashboard.py",
      "use": "@vercel/python"
    },
    {
      "src": "ghl_real_estate_ai/streamlit_components/multimodal_document_intelligence_dashboard.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/intelligence/(.*)",
      "dest": "ghl_real_estate_ai/streamlit_components/realtime_lead_intelligence_hub.py"
    },
    {
      "src": "/coaching/(.*)",
      "dest": "ghl_real_estate_ai/streamlit_components/agent_coaching_dashboard.py"
    },
    {
      "src": "/property-intelligence/(.*)",
      "dest": "ghl_real_estate_ai/streamlit_components/multimodal_document_intelligence_dashboard.py"
    }
  ],
  "env": {
    "RAILWAY_API_URL": "@railway_api_url",
    "REDIS_URL": "@redis_url",
    "DATABASE_URL": "@database_url"
  }
}
```

**Deployment Commands:**

```bash
# 1. Install Vercel CLI
npm install -g vercel

# 2. Login and link project
vercel login
vercel link

# 3. Deploy to production
vercel --prod

# 4. Configure custom domain
vercel domains add intelligence.enterprisehub.ai
vercel domains add coaching.enterprisehub.ai
vercel domains add property.enterprisehub.ai
```

#### 1.3 Database Setup

**PostgreSQL Schema Deployment:**

```sql
-- Phase 3 Analytics Tables

-- Real-time intelligence tracking
CREATE TABLE IF NOT EXISTS realtime_intelligence_events (
    event_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(255) NOT NULL,
    lead_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    intelligence_data JSONB NOT NULL,
    websocket_latency_ms NUMERIC(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_tenant_lead (tenant_id, lead_id),
    INDEX idx_event_type (event_type),
    INDEX idx_created_at (created_at)
);

-- Churn prevention interventions
CREATE TABLE IF NOT EXISTS churn_interventions (
    intervention_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(255) NOT NULL,
    lead_id VARCHAR(255) NOT NULL,
    churn_probability NUMERIC(5,4) NOT NULL,
    intervention_stage VARCHAR(50) NOT NULL,
    channels_used TEXT[] NOT NULL,
    outcome VARCHAR(50),
    detection_latency_ms INTEGER,
    intervention_latency_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_tenant_lead (tenant_id, lead_id),
    INDEX idx_churn_probability (churn_probability),
    INDEX idx_outcome (outcome)
);

-- Coaching sessions
CREATE TABLE IF NOT EXISTS coaching_sessions (
    session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(255) NOT NULL,
    agent_id VARCHAR(255) NOT NULL,
    conversation_id VARCHAR(255),
    analysis_data JSONB NOT NULL,
    coaching_insights JSONB NOT NULL,
    analysis_latency_ms INTEGER,
    improvement_areas TEXT[],
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_tenant_agent (tenant_id, agent_id),
    INDEX idx_created_at (created_at)
);

-- Property vision analysis
CREATE TABLE IF NOT EXISTS property_vision_analyses (
    analysis_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(255) NOT NULL,
    property_id VARCHAR(255) NOT NULL,
    image_url TEXT NOT NULL,
    analysis_results JSONB NOT NULL,
    luxury_score NUMERIC(3,2),
    condition_score NUMERIC(3,2),
    analysis_latency_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_tenant_property (tenant_id, property_id),
    INDEX idx_luxury_score (luxury_score),
    INDEX idx_created_at (created_at)
);

-- Business impact metrics
CREATE TABLE IF NOT EXISTS phase3_business_metrics (
    metric_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(255) NOT NULL,
    metric_date DATE NOT NULL,
    feature_name VARCHAR(100) NOT NULL,
    usage_count INTEGER DEFAULT 0,
    success_count INTEGER DEFAULT 0,
    revenue_impact NUMERIC(12,2) DEFAULT 0,
    performance_p50 NUMERIC(10,2),
    performance_p95 NUMERIC(10,2),
    performance_p99 NUMERIC(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (tenant_id, metric_date, feature_name),
    INDEX idx_feature_date (feature_name, metric_date)
);

-- A/B test assignments
CREATE TABLE IF NOT EXISTS ab_test_assignments (
    assignment_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR(255) NOT NULL,
    user_id VARCHAR(255) NOT NULL,
    feature_name VARCHAR(100) NOT NULL,
    variant VARCHAR(50) NOT NULL, -- 'control' or 'treatment'
    assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (tenant_id, user_id, feature_name)
);
```

**Migration Deployment:**

```bash
# Run migrations
python scripts/run_migrations.py --environment production

# Verify schema
python scripts/verify_database_schema.py
```

#### 1.4 Redis Configuration

**Redis Setup Script:**

```bash
#!/bin/bash
# scripts/setup_redis_production.sh

# Configure Redis for production
redis-cli CONFIG SET maxmemory 2gb
redis-cli CONFIG SET maxmemory-policy allkeys-lru
redis-cli CONFIG SET notify-keyspace-events Ex

# Set up monitoring
redis-cli CONFIG SET slowlog-log-slower-than 10000
redis-cli CONFIG SET slowlog-max-len 128

# Initialize key namespaces
python << EOF
import redis
import json

r = redis.from_url('${REDIS_URL}')

# Initialize feature flags
feature_flags = {
    'realtime_intelligence': {'enabled': False, 'rollout_percentage': 0},
    'property_vision': {'enabled': False, 'rollout_percentage': 0},
    'churn_prevention': {'enabled': False, 'rollout_percentage': 0},
    'ai_coaching': {'enabled': False, 'rollout_percentage': 0}
}

for feature, config in feature_flags.items():
    r.set(f'feature_flag:{feature}', json.dumps(config))

print("Redis configuration complete")
EOF
```

---

## Environment Configuration

### Production Environment Variables

**Required for All Services:**

```bash
# Core Infrastructure
ENVIRONMENT=production
DEPLOYMENT_ENV=production
LOG_LEVEL=INFO

# Database
DATABASE_URL=postgresql://user:pass@postgres-primary.railway.app:5432/enterprisehub
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
DB_POOL_TIMEOUT=30

# Redis
REDIS_URL=redis://redis-cluster.railway.app:6379/0
REDIS_MAX_CONNECTIONS=50
REDIS_SOCKET_TIMEOUT=5

# API Keys
ANTHROPIC_API_KEY=sk-ant-xxxxx
GHL_API_KEY=xxxxx
OPENAI_API_KEY=sk-xxxxx (optional, for embeddings)

# Railway URLs
RAILWAY_API_URL=https://ml-services.railway.app
RAILWAY_WEBSOCKET_URL=wss://websocket-manager.railway.app

# Vercel URLs
VERCEL_INTELLIGENCE_URL=https://intelligence.enterprisehub.ai
VERCEL_COACHING_URL=https://coaching.enterprisehub.ai
VERCEL_PROPERTY_URL=https://property.enterprisehub.ai

# Performance Targets
WEBSOCKET_TARGET_LATENCY_MS=50
ML_INFERENCE_TARGET_MS=35
VISION_ANALYSIS_TARGET_MS=1500
CHURN_INTERVENTION_TARGET_S=30
COACHING_ANALYSIS_TARGET_MS=2000

# Monitoring
SENTRY_DSN=https://xxxxx@sentry.io/xxxxx
DATADOG_API_KEY=xxxxx (optional)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/xxxxx

# Security
JWT_SECRET_KEY=xxxxx (generate with: openssl rand -hex 32)
ENCRYPTION_KEY=xxxxx (generate with: openssl rand -hex 32)
CORS_ORIGINS=https://enterprisehub.ai,https://*.enterprisehub.ai

# Feature Flags (managed in Redis)
# These are defaults, actual values in Redis
FEATURE_REALTIME_INTELLIGENCE=false
FEATURE_PROPERTY_VISION=false
FEATURE_CHURN_PREVENTION=false
FEATURE_AI_COACHING=false
```

**Service-Specific Configuration:**

```bash
# WebSocket Manager
WS_MAX_CONNECTIONS_PER_TENANT=100
WS_HEARTBEAT_INTERVAL=30
WS_RECONNECT_DELAY=5

# ML Services
ML_MODEL_CACHE_SIZE=100
ML_BATCH_SIZE=32
ML_INFERENCE_TIMEOUT=10

# Coaching Engine
COACHING_SESSION_TIMEOUT=300
COACHING_MAX_CONCURRENT_ANALYSES=50

# Churn Orchestrator
CHURN_MONITORING_INTERVAL=60
CHURN_INTERVENTION_RETRY_COUNT=3
```

**Deployment Script:**

```bash
#!/bin/bash
# scripts/deploy_production_env.sh

# Set environment variables in Railway
railway variables set ENVIRONMENT=production
railway variables set DATABASE_URL="$DATABASE_URL"
railway variables set REDIS_URL="$REDIS_URL"
railway variables set ANTHROPIC_API_KEY="$ANTHROPIC_API_KEY"
railway variables set GHL_API_KEY="$GHL_API_KEY"

# Set environment variables in Vercel
vercel env add RAILWAY_API_URL production
vercel env add REDIS_URL production
vercel env add DATABASE_URL production

echo "Environment configuration complete"
```

---

## Deployment Phases

### Phase 1: Infrastructure & Core Services (Days 1-3)

**Objectives:**
- Deploy backend services to Railway
- Deploy frontend dashboards to Vercel
- Configure databases and caching
- Set up monitoring and alerting

**Deployment Steps:**

```bash
# Day 1: Railway Backend
./scripts/deploy_railway_backend.sh

# Day 2: Vercel Frontend
./scripts/deploy_vercel_frontend.sh

# Day 3: Database & Redis
./scripts/setup_production_data_layer.sh
./scripts/verify_infrastructure.sh
```

**Validation Checklist:**

- [ ] All Railway services healthy (4/4 green)
- [ ] All Vercel deployments successful (3/3)
- [ ] Database migrations complete
- [ ] Redis cluster operational
- [ ] Health checks passing
- [ ] Monitoring dashboards live

### Phase 2: Feature Rollout with A/B Testing (Days 4-30)

**Week 1 (Days 4-10): 10% Rollout**

Target: Initial validation with early adopters

```bash
# Enable features for 10% of tenants
python scripts/set_feature_rollout.py \
  --feature realtime_intelligence \
  --percentage 10 \
  --environment production

python scripts/set_feature_rollout.py \
  --feature property_vision \
  --percentage 10 \
  --environment production

python scripts/set_feature_rollout.py \
  --feature churn_prevention \
  --percentage 10 \
  --environment production

python scripts/set_feature_rollout.py \
  --feature ai_coaching \
  --percentage 10 \
  --environment production
```

**Success Criteria (Week 1):**
- [ ] Performance targets met (WebSocket <50ms, Vision <1.5s, etc.)
- [ ] No critical errors or crashes
- [ ] User satisfaction >80%
- [ ] Feature usage >50% of enabled tenants

**Week 2 (Days 11-17): 25% Rollout**

If Week 1 successful, expand to 25% of tenants.

```bash
# Expand rollout
python scripts/set_feature_rollout.py --percentage 25 --all-features
```

**Success Criteria (Week 2):**
- [ ] All Week 1 criteria maintained
- [ ] Business impact visible (conversions, engagement)
- [ ] Support tickets <5% of new users
- [ ] Performance under load validated

**Week 3 (Days 18-24): 50% Rollout**

Expand to half of tenant base.

```bash
# Expand to 50%
python scripts/set_feature_rollout.py --percentage 50 --all-features
```

**Success Criteria (Week 3):**
- [ ] ROI validation showing positive trend
- [ ] System stability under increased load
- [ ] Feature adoption >60%
- [ ] Customer feedback predominantly positive

**Week 4 (Days 25-30): 100% Rollout**

Full production release to all tenants.

```bash
# Full rollout
python scripts/set_feature_rollout.py --percentage 100 --all-features

# Update feature flags to fully enabled
python scripts/enable_all_features.py --environment production
```

**Success Criteria (Week 4):**
- [ ] All business impact metrics validated
- [ ] Performance targets maintained at scale
- [ ] Support operations sustainable
- [ ] Revenue impact measurable

---

## A/B Testing Framework

### Test Configuration

**Randomized Assignment Strategy:**

```python
# scripts/ab_test_manager.py

from typing import Dict, Literal
import hashlib
import random

class ABTestManager:
    """Manages A/B test assignments for Phase 3 features"""

    def __init__(self, redis_client):
        self.redis = redis_client

    def assign_variant(
        self,
        tenant_id: str,
        feature_name: str,
        rollout_percentage: int
    ) -> Literal['control', 'treatment']:
        """
        Assign tenant to control or treatment variant

        Uses deterministic hashing for consistent assignment
        """
        # Check existing assignment
        assignment_key = f"ab_test:{feature_name}:{tenant_id}"
        existing = self.redis.get(assignment_key)

        if existing:
            return existing.decode()

        # Deterministic assignment based on tenant_id hash
        hash_value = int(hashlib.sha256(
            f"{feature_name}:{tenant_id}".encode()
        ).hexdigest(), 16)

        # Assign to treatment if hash % 100 < rollout_percentage
        variant = 'treatment' if (hash_value % 100) < rollout_percentage else 'control'

        # Store assignment
        self.redis.setex(
            assignment_key,
            86400 * 30,  # 30 days
            variant
        )

        # Track in database
        self._record_assignment(tenant_id, feature_name, variant)

        return variant

    def _record_assignment(self, tenant_id: str, feature_name: str, variant: str):
        """Record assignment in database for analysis"""
        from sqlalchemy import insert
        from database import ab_test_assignments

        query = insert(ab_test_assignments).values(
            tenant_id=tenant_id,
            feature_name=feature_name,
            variant=variant
        ).on_conflict_do_nothing()

        # Execute query (implementation depends on your DB setup)
        pass
```

### Metrics Collection

**Key Performance Indicators by Feature:**

```yaml
realtime_intelligence:
  primary_metrics:
    - websocket_latency_p50
    - websocket_latency_p95
    - websocket_latency_p99
    - connection_success_rate
    - event_throughput
  business_metrics:
    - lead_response_time
    - agent_productivity_improvement
    - conversion_rate_lift

property_vision:
  primary_metrics:
    - analysis_latency_p50
    - analysis_latency_p95
    - luxury_detection_accuracy
    - condition_assessment_accuracy
  business_metrics:
    - property_match_satisfaction
    - listing_quality_score
    - time_to_match_reduction

churn_prevention:
  primary_metrics:
    - detection_latency
    - intervention_latency
    - intervention_success_rate
  business_metrics:
    - churn_rate_reduction
    - retention_improvement
    - revenue_saved

ai_coaching:
  primary_metrics:
    - analysis_latency
    - coaching_delivery_time
    - insight_relevance_score
  business_metrics:
    - agent_performance_improvement
    - training_time_reduction
    - skill_development_rate
```

### Analysis Dashboard

**SQL Queries for A/B Test Analysis:**

```sql
-- Feature effectiveness comparison
WITH feature_metrics AS (
    SELECT
        a.feature_name,
        a.variant,
        COUNT(DISTINCT a.tenant_id) as tenant_count,
        AVG(m.success_count::NUMERIC / NULLIF(m.usage_count, 0)) as success_rate,
        AVG(m.revenue_impact) as avg_revenue_impact,
        AVG(m.performance_p50) as avg_latency_p50,
        AVG(m.performance_p95) as avg_latency_p95
    FROM ab_test_assignments a
    JOIN phase3_business_metrics m
        ON a.tenant_id = m.tenant_id
        AND a.feature_name = m.feature_name
    WHERE m.metric_date >= CURRENT_DATE - INTERVAL '7 days'
    GROUP BY a.feature_name, a.variant
)
SELECT
    feature_name,
    variant,
    tenant_count,
    ROUND(success_rate * 100, 2) as success_rate_pct,
    ROUND(avg_revenue_impact, 2) as avg_revenue_impact,
    ROUND(avg_latency_p50, 2) as avg_latency_p50_ms,
    ROUND(avg_latency_p95, 2) as avg_latency_p95_ms
FROM feature_metrics
ORDER BY feature_name, variant;

-- Statistical significance testing
WITH variant_stats AS (
    SELECT
        feature_name,
        variant,
        AVG(revenue_impact) as mean_revenue,
        STDDEV(revenue_impact) as stddev_revenue,
        COUNT(*) as sample_size
    FROM phase3_business_metrics m
    JOIN ab_test_assignments a
        ON m.tenant_id = a.tenant_id
        AND m.feature_name = a.feature_name
    WHERE m.metric_date >= CURRENT_DATE - INTERVAL '7 days'
    GROUP BY feature_name, variant
)
SELECT
    c.feature_name,
    c.mean_revenue as control_revenue,
    t.mean_revenue as treatment_revenue,
    ROUND(((t.mean_revenue - c.mean_revenue) / c.mean_revenue * 100), 2) as lift_percentage,
    -- Simple t-test approximation
    CASE
        WHEN ABS(t.mean_revenue - c.mean_revenue) /
             SQRT((c.stddev_revenue^2 / c.sample_size) +
                  (t.stddev_revenue^2 / t.sample_size)) > 1.96
        THEN 'SIGNIFICANT'
        ELSE 'NOT SIGNIFICANT'
    END as statistical_significance
FROM variant_stats c
JOIN variant_stats t ON c.feature_name = t.feature_name
WHERE c.variant = 'control' AND t.variant = 'treatment';
```

---

## Performance Monitoring

### Real-Time Monitoring Dashboard

**Grafana Dashboard Configuration:**

```yaml
# grafana/phase3_monitoring.json
{
  "dashboard": {
    "title": "Phase 3 Production Monitoring",
    "panels": [
      {
        "title": "WebSocket Performance",
        "targets": [
          {
            "query": "histogram_quantile(0.50, websocket_latency_ms)",
            "legendFormat": "P50"
          },
          {
            "query": "histogram_quantile(0.95, websocket_latency_ms)",
            "legendFormat": "P95"
          },
          {
            "query": "histogram_quantile(0.99, websocket_latency_ms)",
            "legendFormat": "P99"
          }
        ],
        "thresholds": [
          { "value": 50, "color": "green" },
          { "value": 100, "color": "yellow" },
          { "value": 200, "color": "red" }
        ]
      },
      {
        "title": "ML Inference Latency",
        "targets": [
          {
            "query": "histogram_quantile(0.95, ml_inference_latency_ms)",
            "legendFormat": "Lead Intelligence"
          },
          {
            "query": "histogram_quantile(0.95, vision_analysis_latency_ms)",
            "legendFormat": "Property Vision"
          },
          {
            "query": "histogram_quantile(0.95, coaching_analysis_latency_ms)",
            "legendFormat": "Coaching Analysis"
          }
        ]
      },
      {
        "title": "Churn Prevention Metrics",
        "targets": [
          {
            "query": "rate(churn_interventions_delivered[5m])",
            "legendFormat": "Interventions/min"
          },
          {
            "query": "avg(churn_intervention_latency_s)",
            "legendFormat": "Avg Latency"
          },
          {
            "query": "sum(churn_interventions_successful) / sum(churn_interventions_total)",
            "legendFormat": "Success Rate"
          }
        ]
      },
      {
        "title": "Business Impact Metrics",
        "targets": [
          {
            "query": "sum(phase3_revenue_impact_daily)",
            "legendFormat": "Daily Revenue Impact"
          },
          {
            "query": "avg(conversion_rate_lift)",
            "legendFormat": "Conversion Rate Lift %"
          },
          {
            "query": "sum(leads_saved_from_churn)",
            "legendFormat": "Leads Saved"
          }
        ]
      }
    ]
  }
}
```

### Alert Configuration

**Prometheus Alerting Rules:**

```yaml
# prometheus/phase3_alerts.yml
groups:
  - name: phase3_performance
    interval: 30s
    rules:
      # WebSocket Performance
      - alert: WebSocketLatencyHigh
        expr: histogram_quantile(0.95, websocket_latency_ms) > 100
        for: 5m
        labels:
          severity: warning
          component: websocket_manager
        annotations:
          summary: "WebSocket P95 latency above target"
          description: "WebSocket latency {{ $value }}ms exceeds 100ms target"

      # ML Performance
      - alert: MLInferenceSlowdown
        expr: histogram_quantile(0.95, ml_inference_latency_ms) > 50
        for: 5m
        labels:
          severity: warning
          component: ml_services
        annotations:
          summary: "ML inference latency degraded"
          description: "ML P95 latency {{ $value }}ms exceeds 50ms target"

      # Churn Prevention
      - alert: ChurnInterventionLatencyHigh
        expr: avg(churn_intervention_latency_s) > 60
        for: 10m
        labels:
          severity: critical
          component: churn_orchestrator
        annotations:
          summary: "Churn intervention latency exceeds target"
          description: "Average intervention latency {{ $value }}s exceeds 30s target"

      # Business Impact
      - alert: FeatureUsageDrop
        expr: rate(feature_usage_count[1h]) < 0.5 * rate(feature_usage_count[1h] offset 24h)
        for: 30m
        labels:
          severity: warning
          component: business_metrics
        annotations:
          summary: "Feature usage dropped significantly"
          description: "Feature usage down 50% compared to 24h ago"

      # System Health
      - alert: ErrorRateSpiking
        expr: rate(error_count[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "Error rate exceeds 5%"
          description: "Error rate {{ $value }} is above acceptable threshold"
```

### Health Check Endpoints

**Service Health Checks:**

```python
# health_checks.py
from fastapi import APIRouter, Response
from typing import Dict, Any
import asyncio

router = APIRouter()

@router.get("/health")
async def health_check() -> Dict[str, Any]:
    """Overall system health"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@router.get("/health/websocket")
async def websocket_health() -> Dict[str, Any]:
    """WebSocket Manager health"""
    from services.websocket_manager import get_websocket_manager

    manager = get_websocket_manager()
    metrics = await manager.get_health_metrics()

    return {
        "status": "healthy" if metrics["healthy"] else "degraded",
        "active_connections": metrics["connection_count"],
        "avg_latency_ms": metrics["avg_latency_ms"],
        "target_latency_ms": 50,
        "healthy": metrics["avg_latency_ms"] < 100
    }

@router.get("/health/ml")
async def ml_services_health() -> Dict[str, Any]:
    """ML Services health"""
    from services.optimized_ml_lead_intelligence_engine import get_optimized_ml_intelligence_engine

    engine = get_optimized_ml_intelligence_engine()
    metrics = await engine.get_performance_metrics()

    return {
        "status": "healthy" if metrics["p95_latency_ms"] < 50 else "degraded",
        "p50_latency_ms": metrics["p50_latency_ms"],
        "p95_latency_ms": metrics["p95_latency_ms"],
        "target_p95_ms": 35,
        "model_loaded": True
    }

@router.get("/health/coaching")
async def coaching_health() -> Dict[str, Any]:
    """Coaching Engine health"""
    from services.ai_powered_coaching_engine import get_coaching_engine

    engine = get_coaching_engine()
    metrics = await engine.get_health_metrics()

    return {
        "status": "healthy" if metrics["analysis_latency_ms"] < 3000 else "degraded",
        "avg_analysis_latency_ms": metrics["analysis_latency_ms"],
        "target_latency_ms": 2000,
        "active_sessions": metrics["active_sessions"],
        "max_concurrent_sessions": 50
    }

@router.get("/health/churn")
async def churn_health() -> Dict[str, Any]:
    """Churn Orchestrator health"""
    from services.proactive_churn_prevention_orchestrator import get_churn_orchestrator

    orchestrator = get_churn_orchestrator()
    metrics = await orchestrator.get_health_metrics()

    return {
        "status": "healthy" if metrics["intervention_latency_s"] < 60 else "degraded",
        "avg_intervention_latency_s": metrics["intervention_latency_s"],
        "target_latency_s": 30,
        "interventions_24h": metrics["interventions_count"],
        "success_rate": metrics["success_rate"]
    }

@router.get("/health/detailed")
async def detailed_health() -> Dict[str, Any]:
    """Comprehensive health check across all services"""

    checks = await asyncio.gather(
        websocket_health(),
        ml_services_health(),
        coaching_health(),
        churn_health(),
        return_exceptions=True
    )

    overall_healthy = all(
        not isinstance(c, Exception) and c.get("status") == "healthy"
        for c in checks
    )

    return {
        "status": "healthy" if overall_healthy else "degraded",
        "services": {
            "websocket": checks[0],
            "ml": checks[1],
            "coaching": checks[2],
            "churn": checks[3]
        },
        "timestamp": datetime.now().isoformat()
    }
```

---

## Business Impact Measurement

### Revenue Tracking Framework

**Daily Revenue Impact Calculation:**

```python
# scripts/calculate_business_impact.py

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List
import asyncio

@dataclass
class FeatureImpact:
    """Business impact metrics for a feature"""
    feature_name: str
    date: datetime

    # Usage metrics
    active_tenants: int
    total_usage_count: int
    successful_operations: int

    # Performance metrics
    avg_latency_ms: float
    p95_latency_ms: float

    # Business metrics
    revenue_impact: float
    conversion_lift: float
    time_saved_hours: float

    # ROI calculation
    operating_cost: float  # Infrastructure + API costs
    net_impact: float  # revenue_impact - operating_cost

class BusinessImpactCalculator:
    """Calculate daily business impact for Phase 3 features"""

    def __init__(self, db_session, redis_client):
        self.db = db_session
        self.redis = redis_client

    async def calculate_daily_impact(self, date: datetime) -> Dict[str, FeatureImpact]:
        """Calculate impact for all features for a specific date"""

        features = [
            'realtime_intelligence',
            'property_vision',
            'churn_prevention',
            'ai_coaching'
        ]

        impacts = {}
        for feature in features:
            impacts[feature] = await self._calculate_feature_impact(feature, date)

        return impacts

    async def _calculate_feature_impact(
        self,
        feature_name: str,
        date: datetime
    ) -> FeatureImpact:
        """Calculate impact for a specific feature"""

        # Get usage metrics from database
        usage_query = f"""
            SELECT
                COUNT(DISTINCT tenant_id) as active_tenants,
                SUM(usage_count) as total_usage,
                SUM(success_count) as successful_ops,
                AVG(performance_p50) as avg_latency,
                AVG(performance_p95) as p95_latency
            FROM phase3_business_metrics
            WHERE feature_name = '{feature_name}'
            AND metric_date = '{date.date()}'
        """

        usage_metrics = await self.db.execute(usage_query).first()

        # Calculate revenue impact based on feature
        revenue_impact = await self._calculate_revenue_impact(
            feature_name,
            date,
            usage_metrics
        )

        # Calculate operating costs
        operating_cost = await self._calculate_operating_cost(
            feature_name,
            usage_metrics.total_usage
        )

        return FeatureImpact(
            feature_name=feature_name,
            date=date,
            active_tenants=usage_metrics.active_tenants,
            total_usage_count=usage_metrics.total_usage,
            successful_operations=usage_metrics.successful_ops,
            avg_latency_ms=usage_metrics.avg_latency,
            p95_latency_ms=usage_metrics.p95_latency,
            revenue_impact=revenue_impact,
            conversion_lift=await self._calculate_conversion_lift(feature_name, date),
            time_saved_hours=await self._calculate_time_saved(feature_name, usage_metrics),
            operating_cost=operating_cost,
            net_impact=revenue_impact - operating_cost
        )

    async def _calculate_revenue_impact(
        self,
        feature_name: str,
        date: datetime,
        usage_metrics
    ) -> float:
        """Calculate revenue impact by feature type"""

        if feature_name == 'realtime_intelligence':
            # Revenue from faster lead response and higher conversion
            # Assumption: 5% conversion lift, $50 avg commission per lead
            leads_affected = usage_metrics.total_usage
            conversion_improvement = 0.05  # 5% lift
            avg_commission = 50.0
            return leads_affected * conversion_improvement * avg_commission

        elif feature_name == 'property_vision':
            # Revenue from better property matching and satisfaction
            # Assumption: 3% higher match rate, $75 avg value per match
            matches_affected = usage_metrics.total_usage
            match_improvement = 0.03
            avg_match_value = 75.0
            return matches_affected * match_improvement * avg_match_value

        elif feature_name == 'churn_prevention':
            # Revenue from saved leads that would have churned
            # Assumption: 15% churn reduction, $100 avg lead lifetime value
            interventions = usage_metrics.successful_ops
            churn_reduction_rate = 0.15
            avg_lead_ltv = 100.0
            return interventions * churn_reduction_rate * avg_lead_ltv

        elif feature_name == 'ai_coaching':
            # Revenue from improved agent productivity
            # Assumption: 10% productivity increase, $200 avg daily revenue per agent
            coaching_sessions = usage_metrics.total_usage
            productivity_increase = 0.10
            avg_daily_revenue = 200.0
            return coaching_sessions * productivity_increase * avg_daily_revenue

        return 0.0

    async def _calculate_operating_cost(
        self,
        feature_name: str,
        usage_count: int
    ) -> float:
        """Calculate operating costs (infrastructure + API calls)"""

        # Infrastructure costs (daily allocation)
        infra_costs = {
            'realtime_intelligence': 15.0,  # $450/month / 30
            'property_vision': 20.0,  # $600/month / 30 (higher for vision API)
            'churn_prevention': 10.0,  # $300/month / 30
            'ai_coaching': 15.0  # $450/month / 30
        }

        # API costs per usage
        api_costs = {
            'realtime_intelligence': 0.001,  # $0.001 per intelligence generation
            'property_vision': 0.05,  # $0.05 per image analysis (Claude Vision)
            'churn_prevention': 0.002,  # $0.002 per intervention
            'ai_coaching': 0.01  # $0.01 per coaching session (Claude analysis)
        }

        infra_cost = infra_costs.get(feature_name, 0.0)
        api_cost = api_costs.get(feature_name, 0.0) * usage_count

        return infra_cost + api_cost

    async def _calculate_conversion_lift(self, feature_name: str, date: datetime) -> float:
        """Calculate conversion rate lift from A/B test data"""

        query = f"""
            WITH conversion_rates AS (
                SELECT
                    a.variant,
                    COUNT(DISTINCT CASE WHEN m.success_count > 0 THEN m.tenant_id END)::NUMERIC /
                    NULLIF(COUNT(DISTINCT m.tenant_id), 0) as conversion_rate
                FROM ab_test_assignments a
                JOIN phase3_business_metrics m
                    ON a.tenant_id = m.tenant_id
                    AND a.feature_name = m.feature_name
                WHERE a.feature_name = '{feature_name}'
                AND m.metric_date = '{date.date()}'
                GROUP BY a.variant
            )
            SELECT
                (MAX(CASE WHEN variant = 'treatment' THEN conversion_rate ELSE 0 END) -
                 MAX(CASE WHEN variant = 'control' THEN conversion_rate ELSE 0 END)) /
                NULLIF(MAX(CASE WHEN variant = 'control' THEN conversion_rate ELSE 1 END), 0) as lift
            FROM conversion_rates
        """

        result = await self.db.execute(query).first()
        return result.lift if result and result.lift else 0.0

    async def _calculate_time_saved(self, feature_name: str, usage_metrics) -> float:
        """Calculate time saved in hours"""

        time_savings_per_use = {
            'realtime_intelligence': 0.05,  # 3 minutes saved per real-time update
            'property_vision': 0.083,  # 5 minutes saved per property analysis
            'churn_prevention': 0.25,  # 15 minutes saved per automated intervention
            'ai_coaching': 0.5  # 30 minutes saved per coaching session
        }

        time_per_use = time_savings_per_use.get(feature_name, 0.0)
        return usage_metrics.total_usage * time_per_use

    async def generate_weekly_report(self, start_date: datetime) -> Dict:
        """Generate comprehensive weekly business impact report"""

        week_impacts = []
        for day in range(7):
            date = start_date + timedelta(days=day)
            daily_impact = await self.calculate_daily_impact(date)
            week_impacts.append(daily_impact)

        # Aggregate weekly metrics
        weekly_summary = {
            'period': f"{start_date.date()} to {(start_date + timedelta(days=6)).date()}",
            'features': {}
        }

        for feature in ['realtime_intelligence', 'property_vision', 'churn_prevention', 'ai_coaching']:
            feature_impacts = [day[feature] for day in week_impacts]

            weekly_summary['features'][feature] = {
                'total_revenue_impact': sum(f.revenue_impact for f in feature_impacts),
                'total_operating_cost': sum(f.operating_cost for f in feature_impacts),
                'net_impact': sum(f.net_impact for f in feature_impacts),
                'total_usage': sum(f.total_usage_count for f in feature_impacts),
                'avg_latency_ms': sum(f.avg_latency_ms for f in feature_impacts) / len(feature_impacts),
                'time_saved_hours': sum(f.time_saved_hours for f in feature_impacts),
                'avg_conversion_lift': sum(f.conversion_lift for f in feature_impacts) / len(feature_impacts)
            }

        # Calculate total platform impact
        weekly_summary['total_platform'] = {
            'revenue_impact': sum(
                f['total_revenue_impact']
                for f in weekly_summary['features'].values()
            ),
            'operating_cost': sum(
                f['total_operating_cost']
                for f in weekly_summary['features'].values()
            ),
            'net_impact': sum(
                f['net_impact']
                for f in weekly_summary['features'].values()
            ),
            'roi_percentage': (
                sum(f['total_revenue_impact'] for f in weekly_summary['features'].values()) /
                sum(f['total_operating_cost'] for f in weekly_summary['features'].values()) * 100
            ) if sum(f['total_operating_cost'] for f in weekly_summary['features'].values()) > 0 else 0
        }

        return weekly_summary

# Deployment script
async def main():
    """Run daily business impact calculation"""

    from database import get_db_session
    from ghl_real_estate_ai.database.redis_client import redis_client

    calculator = BusinessImpactCalculator(get_db_session(), redis_client)

    # Calculate for yesterday
    yesterday = datetime.now() - timedelta(days=1)
    impacts = await calculator.calculate_daily_impact(yesterday)

    # Store results
    for feature_name, impact in impacts.items():
        print(f"\n{feature_name.upper()}:")
        print(f"  Revenue Impact: ${impact.revenue_impact:.2f}")
        print(f"  Operating Cost: ${impact.operating_cost:.2f}")
        print(f"  Net Impact: ${impact.net_impact:.2f}")
        print(f"  Usage Count: {impact.total_usage_count}")
        print(f"  Conversion Lift: {impact.conversion_lift*100:.2f}%")
        print(f"  Time Saved: {impact.time_saved_hours:.1f} hours")

    # Generate weekly report
    week_start = datetime.now() - timedelta(days=7)
    weekly_report = await calculator.generate_weekly_report(week_start)

    print("\n" + "="*60)
    print("WEEKLY SUMMARY")
    print("="*60)
    print(f"\nTotal Platform Impact:")
    print(f"  Revenue: ${weekly_report['total_platform']['revenue_impact']:.2f}")
    print(f"  Cost: ${weekly_report['total_platform']['operating_cost']:.2f}")
    print(f"  Net: ${weekly_report['total_platform']['net_impact']:.2f}")
    print(f"  ROI: {weekly_report['total_platform']['roi_percentage']:.1f}%")

if __name__ == '__main__':
    asyncio.run(main())
```

### ROI Dashboard

**Streamlit ROI Visualization:**

```python
# streamlit_components/phase3_roi_dashboard.py

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from datetime import datetime, timedelta

st.set_page_config(
    page_title="Phase 3 ROI Dashboard",
    page_icon="ðŸ’°",
    layout="wide"
)

st.title("Phase 3 Business Impact Dashboard")
st.markdown("Real-time revenue tracking and ROI analysis for all Phase 3 features")

# Date range selector
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("Start Date", datetime.now() - timedelta(days=30))
with col2:
    end_date = st.date_input("End Date", datetime.now())

# Fetch business impact data
@st.cache_data(ttl=300)
def load_business_impact(start, end):
    # Query phase3_business_metrics table
    query = f"""
        SELECT
            metric_date,
            feature_name,
            SUM(usage_count) as usage,
            SUM(revenue_impact) as revenue,
            AVG(performance_p95) as latency_p95
        FROM phase3_business_metrics
        WHERE metric_date BETWEEN '{start}' AND '{end}'
        GROUP BY metric_date, feature_name
        ORDER BY metric_date
    """
    # Execute and return as DataFrame
    # Implementation depends on your DB setup
    return pd.DataFrame()  # Placeholder

df = load_business_impact(start_date, end_date)

# Key metrics at top
st.header("Summary Metrics")
col1, col2, col3, col4 = st.columns(4)

total_revenue = df['revenue'].sum()
avg_latency = df['latency_p95'].mean()
total_usage = df['usage'].sum()

# Estimate operating cost (simplified)
operating_cost = total_usage * 0.01  # $0.01 avg per operation
net_impact = total_revenue - operating_cost
roi = (net_impact / operating_cost * 100) if operating_cost > 0 else 0

col1.metric("Total Revenue Impact", f"${total_revenue:,.0f}")
col2.metric("Operating Cost", f"${operating_cost:,.0f}")
col3.metric("Net Impact", f"${net_impact:,.0f}", delta=f"{roi:.1f}% ROI")
col4.metric("Avg Performance (P95)", f"{avg_latency:.1f}ms")

# Revenue trend over time
st.header("Revenue Trend by Feature")
fig = go.Figure()

for feature in df['feature_name'].unique():
    feature_data = df[df['feature_name'] == feature]
    fig.add_trace(go.Scatter(
        x=feature_data['metric_date'],
        y=feature_data['revenue'],
        mode='lines+markers',
        name=feature.replace('_', ' ').title()
    ))

fig.update_layout(
    xaxis_title="Date",
    yaxis_title="Revenue Impact ($)",
    hovermode='x unified',
    height=400
)
st.plotly_chart(fig, use_container_width=True)

# Feature breakdown
st.header("Feature Performance Breakdown")
feature_summary = df.groupby('feature_name').agg({
    'usage': 'sum',
    'revenue': 'sum',
    'latency_p95': 'mean'
}).reset_index()

feature_summary['feature_display'] = feature_summary['feature_name'].str.replace('_', ' ').str.title()

col1, col2 = st.columns(2)

with col1:
    # Usage distribution pie chart
    fig_usage = go.Figure(data=[go.Pie(
        labels=feature_summary['feature_display'],
        values=feature_summary['usage'],
        hole=0.3
    )])
    fig_usage.update_layout(title="Usage Distribution", height=350)
    st.plotly_chart(fig_usage, use_container_width=True)

with col2:
    # Revenue distribution pie chart
    fig_revenue = go.Figure(data=[go.Pie(
        labels=feature_summary['feature_display'],
        values=feature_summary['revenue'],
        hole=0.3
    )])
    fig_revenue.update_layout(title="Revenue Distribution", height=350)
    st.plotly_chart(fig_revenue, use_container_width=True)

# Performance vs Target
st.header("Performance vs Targets")

targets = {
    'realtime_intelligence': 50,
    'property_vision': 1500,
    'churn_prevention': 1000,  # 30s = 30000ms, showing avg here
    'ai_coaching': 2000
}

perf_df = feature_summary[['feature_display', 'latency_p95']].copy()
perf_df['target'] = perf_df['feature_display'].map(
    lambda x: targets.get(x.lower().replace(' ', '_'), 0)
)
perf_df['status'] = perf_df.apply(
    lambda row: 'âœ… Meeting Target' if row['latency_p95'] <= row['target'] else 'âš ï¸ Above Target',
    axis=1
)

st.dataframe(
    perf_df.rename(columns={
        'feature_display': 'Feature',
        'latency_p95': 'Actual P95 (ms)',
        'target': 'Target (ms)',
        'status': 'Status'
    }),
    hide_index=True,
    use_container_width=True
)

# Projected annual impact
st.header("Projected Annual Impact")
st.markdown("""
Based on current performance, here are the projected annual values:
""")

daily_avg_revenue = total_revenue / max((end_date - start_date).days, 1)
projected_annual = daily_avg_revenue * 365

col1, col2, col3 = st.columns(3)
col1.metric("Current Daily Avg", f"${daily_avg_revenue:,.0f}")
col2.metric("Projected Annual", f"${projected_annual:,.0f}")
col3.metric("vs Target ($265K-440K)",
            f"{(projected_annual / 350000 * 100):.0f}% of midpoint")

# Add export functionality
if st.button("Export Data to CSV"):
    csv = df.to_csv(index=False)
    st.download_button(
        label="Download CSV",
        data=csv,
        file_name=f"phase3_impact_{start_date}_{end_date}.csv",
        mime="text/csv"
    )
```

---

## Rollback Procedures

### Automated Rollback Triggers

**Rollback Conditions:**

```yaml
# rollback_config.yml
rollback_triggers:
  performance_degradation:
    - metric: websocket_latency_p95
      threshold: 200  # ms
      duration: 10m
      action: rollback_realtime_intelligence

    - metric: ml_inference_latency_p95
      threshold: 100  # ms
      duration: 10m
      action: rollback_ml_services

    - metric: vision_analysis_latency_p95
      threshold: 3000  # ms
      duration: 5m
      action: rollback_property_vision

  error_rate:
    - metric: error_rate_percentage
      threshold: 10  # %
      duration: 5m
      action: rollback_all_features

    - metric: websocket_connection_failures
      threshold: 25  # %
      duration: 5m
      action: rollback_websocket_manager

  business_impact:
    - metric: conversion_rate_drop
      threshold: -15  # % decrease
      duration: 1h
      action: alert_and_investigate

    - metric: feature_usage_drop
      threshold: -50  # % decrease
      duration: 30m
      action: alert_and_investigate
```

### Rollback Script

```bash
#!/bin/bash
# scripts/rollback_phase3.sh

set -e

FEATURE=$1
ENVIRONMENT=${2:-production}

if [ -z "$FEATURE" ]; then
    echo "Usage: ./rollback_phase3.sh <feature_name> [environment]"
    echo "Features: realtime_intelligence, property_vision, churn_prevention, ai_coaching, all"
    exit 1
fi

echo "=========================================="
echo "Phase 3 Feature Rollback"
echo "Feature: $FEATURE"
echo "Environment: $ENVIRONMENT"
echo "=========================================="

# Confirm rollback
read -p "Are you sure you want to rollback $FEATURE? (yes/no): " confirm
if [ "$confirm" != "yes" ]; then
    echo "Rollback cancelled"
    exit 0
fi

# Disable feature flags
echo "Step 1: Disabling feature flags..."
python << EOF
import redis
import json

r = redis.from_url('${REDIS_URL}')

features_to_disable = []
if '$FEATURE' == 'all':
    features_to_disable = [
        'realtime_intelligence',
        'property_vision',
        'churn_prevention',
        'ai_coaching'
    ]
else:
    features_to_disable = ['$FEATURE']

for feature in features_to_disable:
    config = {'enabled': False, 'rollout_percentage': 0}
    r.set(f'feature_flag:{feature}', json.dumps(config))
    print(f"Disabled {feature}")

print("Feature flags disabled successfully")
EOF

# Stop affected services
echo "Step 2: Stopping affected services..."
if [ "$FEATURE" == "all" ] || [ "$FEATURE" == "realtime_intelligence" ]; then
    railway service stop websocket-manager --environment $ENVIRONMENT
fi

if [ "$FEATURE" == "all" ] || [ "$FEATURE" == "property_vision" ] || [ "$FEATURE" == "churn_prevention" ] || [ "$FEATURE" == "ai_coaching" ]; then
    railway service stop ml-services --environment $ENVIRONMENT
fi

# Restore previous deployment
echo "Step 3: Restoring previous deployment..."
PREVIOUS_DEPLOYMENT=$(railway deployment list --environment $ENVIRONMENT --json | jq -r '.[1].id')

echo "Rolling back to deployment: $PREVIOUS_DEPLOYMENT"
railway deployment rollback $PREVIOUS_DEPLOYMENT --environment $ENVIRONMENT

# Wait for services to stabilize
echo "Step 4: Waiting for services to stabilize..."
sleep 30

# Verify rollback
echo "Step 5: Verifying rollback..."
python scripts/verify_rollback.py --feature $FEATURE --environment $ENVIRONMENT

# Send notifications
echo "Step 6: Sending rollback notifications..."
python << EOF
import requests
import os

slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
if slack_webhook:
    message = {
        'text': f':warning: Phase 3 Rollback Executed',
        'blocks': [
            {
                'type': 'section',
                'text': {
                    'type': 'mrkdwn',
                    'text': f'*Feature:* $FEATURE\n*Environment:* $ENVIRONMENT\n*Status:* Rollback Complete'
                }
            }
        ]
    }
    requests.post(slack_webhook, json=message)
    print("Slack notification sent")
EOF

echo "=========================================="
echo "Rollback complete!"
echo "=========================================="
echo ""
echo "Next steps:"
echo "1. Monitor system health: railway logs --environment $ENVIRONMENT"
echo "2. Review rollback metrics: python scripts/analyze_rollback_impact.py"
echo "3. Investigate root cause before re-enabling feature"
```

---

## Security & Compliance

### API Key Management

```bash
# Rotate API keys (run monthly or after suspected breach)
#!/bin/bash
# scripts/rotate_api_keys.sh

echo "Rotating API keys for Phase 3 services..."

# Generate new keys
NEW_JWT_SECRET=$(openssl rand -hex 32)
NEW_ENCRYPTION_KEY=$(openssl rand -hex 32)

# Update Railway environment
railway variables set JWT_SECRET_KEY="$NEW_JWT_SECRET" --environment production
railway variables set ENCRYPTION_KEY="$NEW_ENCRYPTION_KEY" --environment production

# Update Vercel environment
vercel env rm JWT_SECRET_KEY production -y
vercel env add JWT_SECRET_KEY production < <(echo "$NEW_JWT_SECRET")

# Restart services to pick up new keys
railway service restart --all --environment production

echo "API key rotation complete. New keys deployed."
```

### Data Privacy Compliance

**GDPR/CCPA Compliance Checklist:**

- [ ] Lead data encrypted at rest and in transit
- [ ] PII anonymization in analytics
- [ ] Right to deletion implemented
- [ ] Data retention policies configured (30 days for events, 1 year for aggregated metrics)
- [ ] User consent tracking enabled
- [ ] Data export functionality available
- [ ] Third-party data sharing documented

---

## Success Criteria

### Phase 3 Production Success Metrics

**Technical Performance:**

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| WebSocket Latency (P95) | <50ms | Prometheus histogram |
| ML Inference Latency (P95) | <35ms | Service metrics |
| Vision Analysis Time (P95) | <1.5s | Service metrics |
| Churn Intervention Latency | <30s | End-to-end tracking |
| Coaching Analysis Time | <2s | Service metrics |
| System Uptime | >99.9% | Health check logs |
| Error Rate | <1% | Error tracking |

**Business Impact:**

| Feature | Target Annual Value | Success Indicator |
|---------|-------------------|-------------------|
| Real-Time Intelligence | $75K-120K | 10% conversion lift, 20% faster response time |
| Property Vision | $75K-150K | 5% higher match satisfaction, 15% more qualified matches |
| Churn Prevention | $55K-80K | 40% churn reduction (35% â†’ 21%), 65% intervention success |
| AI Coaching | $60K-90K | 25% agent productivity increase, 50% training time reduction |

**Adoption Metrics:**

- [ ] 80%+ feature adoption rate among enabled tenants
- [ ] <2% support ticket rate for new features
- [ ] 85%+ user satisfaction score (NPS >50)
- [ ] 60%+ daily active usage rate

**ROI Validation:**

- [ ] Positive net impact within 30 days
- [ ] 500%+ ROI within 90 days
- [ ] Revenue impact measurable and attributable
- [ ] Operating costs within budget (<20% of revenue impact)

---

## Deployment Timeline

### 30-Day Rollout Schedule

```
Week 1 (Days 1-7): Infrastructure & 10% Rollout
â”œâ”€ Day 1-3: Deploy infrastructure (Railway + Vercel + DB)
â”œâ”€ Day 4-5: Enable features for 10% of tenants
â”œâ”€ Day 6-7: Monitor and validate initial performance
â””â”€ Milestone: All performance targets met, no critical issues

Week 2 (Days 8-14): 25% Rollout & Optimization
â”œâ”€ Day 8-9: Expand to 25% rollout
â”œâ”€ Day 10-12: A/B test analysis and optimization
â”œâ”€ Day 13-14: Business impact measurement begins
â””â”€ Milestone: Positive business metrics visible

Week 3 (Days 15-21): 50% Rollout & Scaling
â”œâ”€ Day 15-16: Expand to 50% rollout
â”œâ”€ Day 17-19: Validate performance at scale
â”œâ”€ Day 20-21: ROI validation and reporting
â””â”€ Milestone: ROI validated, system stable at scale

Week 4 (Days 22-30): 100% Rollout & Production
â”œâ”€ Day 22-24: Expand to 100% rollout
â”œâ”€ Day 25-27: Full production monitoring
â”œâ”€ Day 28-30: Documentation and handoff
â””â”€ Milestone: Full production deployment complete
```

---

## Appendix

### Deployment Checklist

**Pre-Deployment (Days -7 to -1):**

- [ ] All Phase 3 features code complete and tested
- [ ] Performance benchmarks validated in staging
- [ ] Database migration scripts tested
- [ ] Monitoring dashboards configured
- [ ] Alert rules defined and tested
- [ ] Rollback procedures documented and tested
- [ ] Team trained on new features
- [ ] Support documentation complete
- [ ] Customer communication prepared

**Deployment Day (Day 0):**

- [ ] Infrastructure deployed to Railway
- [ ] Dashboards deployed to Vercel
- [ ] Database migrations executed
- [ ] Redis configured
- [ ] Environment variables set
- [ ] Health checks passing
- [ ] Feature flags set to 0% (disabled)
- [ ] Monitoring confirmed operational

**Post-Deployment (Days 1-30):**

- [ ] Progressive rollout executed (10% â†’ 25% â†’ 50% â†’ 100%)
- [ ] A/B tests running and analyzed
- [ ] Performance metrics monitored continuously
- [ ] Business impact tracked daily
- [ ] Support tickets addressed promptly
- [ ] Weekly reports generated and reviewed
- [ ] ROI validated within 30 days

### Contact & Escalation

**Deployment Team:**
- **Technical Lead:** [Name]
- **DevOps:** [Name]
- **Product Manager:** [Name]
- **Support Lead:** [Name]

**Escalation Path:**
1. On-call engineer (24/7): [Contact]
2. Technical lead: [Contact]
3. VP Engineering: [Contact]

**Communication Channels:**
- **Slack:** #phase3-deployment
- **PagerDuty:** Phase 3 Production Alerts
- **Status Page:** status.enterprisehub.ai

---

**Document Status:** Ready for Production Deployment
**Next Review:** After Week 1 completion
**Success Measurement:** 30-day business impact validation

