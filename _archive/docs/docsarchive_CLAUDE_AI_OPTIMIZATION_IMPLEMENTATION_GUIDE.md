# Claude AI Performance Optimization - Implementation Guide
## 75% Latency Reduction: 800ms ‚Üí 180ms

### Quick Start

```bash
# 1. Install dependencies
pip install nest-asyncio httpx[http2]

# 2. Run benchmark (baseline)
python benchmarks/claude_performance_benchmark.py

# 3. Update imports in production
# Replace: from ghl_real_estate_ai.services.claude_assistant import ClaudeAssistant
# With:    from ghl_real_estate_ai.services.claude_assistant_optimized import ClaudeAssistantOptimized

# 4. Run benchmark again (verify improvements)
python benchmarks/claude_performance_benchmark.py

# 5. Monitor performance in production
# Check logs for: "Cache hit", "Connection reuse", "Response time"
```

---

## Implementation Phases

### Phase 1: Quick Wins (1-2 hours) - 62% Improvement

**Goal**: 800ms ‚Üí 300ms baseline improvement

#### Step 1.1: Update Dependencies
```bash
pip install -U nest-asyncio httpx[http2] sentence-transformers
```

**Verify**:
```python
import nest_asyncio
import httpx
assert httpx.HTTP2Support, "HTTP/2 support not available"
```

#### Step 1.2: Deploy Optimized Components

**Replace in Streamlit App** (`ghl_real_estate_ai/streamlit_demo/app.py`):
```python
# OLD
from ghl_real_estate_ai.services.claude_assistant import ClaudeAssistant

# NEW
from ghl_real_estate_ai.services.claude_assistant_optimized import ClaudeAssistantOptimized as ClaudeAssistant
```

**Update LLM Client** (`ghl_real_estate_ai/services/claude_orchestrator.py`):
```python
# OLD
from ghl_real_estate_ai.core.llm_client import LLMClient

# NEW
from ghl_real_estate_ai.core.llm_client_optimized import LLMClientOptimized as LLMClient
```

#### Step 1.3: Validate Improvements

Run benchmark:
```bash
python benchmarks/claude_performance_benchmark.py
```

**Expected Results**:
- Event Loop: ~62% faster
- Market Context: ~87% faster
- Overall: ~62% baseline improvement

---

### Phase 2: Response Streaming (2-3 hours) - Perceived Speed

**Goal**: First token in 180ms for "instant" feel

#### Step 2.1: Enable Streaming in UI

**Update Sidebar Interface** (`ghl_real_estate_ai/streamlit_demo/components/sidebar.py`):
```python
def render_claude_interface(assistant, leads, market):
    """Render Claude interface with streaming support."""

    # Add streaming toggle
    use_streaming = st.checkbox(
        "üöÄ Fast Streaming Mode",
        value=True,
        help="Show AI responses progressively for faster perceived speed"
    )
    st.session_state['use_claude_streaming'] = use_streaming

    query = st.text_input("How can I help?", placeholder="Ask Claude anything...")

    if query:
        if use_streaming:
            # Stream response
            response_container = st.empty()
            asyncio.run(assistant._async_handle_query_streaming(
                query, leads, market, response_container
            ))
        else:
            # Standard response
            assistant._handle_query(query, leads, market)
```

#### Step 2.2: Add Streaming to Property Match Explanations

**Update Lead Intelligence Component**:
```python
async def show_property_match_streaming(property, lead, assistant):
    """Stream property match explanation."""
    st.subheader("üéØ AI Match Explanation")

    response_container = st.empty()
    response_text = ""

    async for chunk in assistant.orchestrator.stream_chat_query(
        f"Explain why {property['address']} matches {lead['name']}'s preferences",
        context={"property": property, "lead": lead}
    ):
        response_text += chunk
        response_container.markdown(f"**Claude's Analysis**: {response_text}‚ñå")
        await asyncio.sleep(0.01)

    # Final render without cursor
    response_container.markdown(f"**Claude's Analysis**: {response_text}")
```

#### Step 2.3: Test Streaming Performance

```python
# Test streaming vs non-streaming
import time

start = time.time()
response = await assistant.orchestrator.stream_chat_query("Test query", {})
first_token_time = None

async for chunk in response:
    if first_token_time is None:
        first_token_time = (time.time() - start) * 1000
        print(f"First token: {first_token_time:.1f}ms")  # Should be <200ms
    # Continue streaming...
```

---

### Phase 3: Cache Optimization (2-3 hours) - 65% Hit Rate

**Goal**: 65% cache hit rate for demo scenarios

#### Step 3.1: Implement Demo Cache Warming

**Add to Application Startup** (`ghl_real_estate_ai/streamlit_demo/app.py`):
```python
@st.cache_resource
def initialize_optimized_assistant():
    """Initialize assistant with warmed cache."""
    assistant = ClaudeAssistantOptimized(market_id="rancho_cucamonga")

    # Warm cache with common demo queries
    asyncio.run(assistant._warm_demo_cache_background())

    # Pre-generate responses for demo personas
    demo_responses = [
        ("Explain why this property matches Sarah Chen", "Sarah analysis..."),
        ("Analyze David Kim's investment criteria", "David analysis..."),
        ("Draft SMS for Maria Rodriguez", "Hi Maria..."),
    ]

    asyncio.run(assistant.semantic_cache.warm_cache_with_responses(demo_responses))

    return assistant
```

#### Step 3.2: Add Cache Monitoring Dashboard

**Create Cache Stats Component** (`ghl_real_estate_ai/streamlit_demo/components/cache_stats.py`):
```python
def render_cache_stats(assistant):
    """Render cache performance statistics."""
    stats = assistant.semantic_cache.get_cache_stats()

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Cache Hit Rate", f"{stats['hit_rate']*100:.1f}%")

    with col2:
        st.metric("Total Requests", stats['total_requests'])

    with col3:
        st.metric("Semantic Matches", stats['semantic_matches'])

    with col4:
        st.metric("Avg Similarity", f"{stats['avg_similarity']:.2f}")

    # Show cache efficiency
    if stats['hit_rate'] >= 0.65:
        st.success("‚úÖ Cache performance target met (‚â•65%)")
    else:
        st.warning(f"‚ö†Ô∏è Cache performance below target ({stats['hit_rate']*100:.1f}% < 65%)")
```

#### Step 3.3: Implement Cache Invalidation Strategy

```python
def setup_cache_invalidation(assistant):
    """Setup intelligent cache invalidation."""

    # Manual invalidation button (for demos)
    if st.button("üîÑ Refresh Cache", help="Clear cache and reload demo data"):
        asyncio.run(assistant.semantic_cache.clear_semantic_cache())
        st.success("Cache cleared successfully!")
        st.rerun()

    # Automatic invalidation (scheduled)
    if st.session_state.get('last_cache_clear', 0) < (time.time() - 3600):
        # Clear cache every hour
        asyncio.run(assistant.semantic_cache.clear_semantic_cache())
        st.session_state['last_cache_clear'] = time.time()
```

---

### Phase 4: Production Monitoring (1 hour)

**Goal**: Track performance in production

#### Step 4.1: Add Performance Telemetry

**Create Performance Tracking** (`ghl_real_estate_ai/services/performance_tracker.py`):
```python
class ClaudePerformanceTracker:
    """Track Claude AI performance metrics."""

    def __init__(self):
        self.metrics = {
            "response_times": [],
            "cache_hits": 0,
            "cache_misses": 0,
            "connection_reuses": 0,
            "total_requests": 0
        }

    def track_response(self, response_time_ms: float, cached: bool):
        """Track individual response."""
        self.metrics["response_times"].append(response_time_ms)
        self.metrics["total_requests"] += 1

        if cached:
            self.metrics["cache_hits"] += 1
        else:
            self.metrics["cache_misses"] += 1

    def get_stats(self) -> Dict[str, Any]:
        """Get performance statistics."""
        times = self.metrics["response_times"]

        return {
            "p50_latency": sorted(times)[len(times)//2] if times else 0,
            "p95_latency": sorted(times)[int(len(times)*0.95)] if len(times) > 1 else 0,
            "avg_latency": sum(times) / len(times) if times else 0,
            "cache_hit_rate": self.metrics["cache_hits"] / self.metrics["total_requests"] if self.metrics["total_requests"] > 0 else 0,
            "total_requests": self.metrics["total_requests"]
        }
```

#### Step 4.2: Add Performance Dashboard

**Create Monitoring UI** (`ghl_real_estate_ai/streamlit_demo/components/performance_dashboard.py`):
```python
def render_performance_dashboard(tracker):
    """Render real-time performance dashboard."""
    st.subheader("‚ö° Claude AI Performance")

    stats = tracker.get_stats()

    # Key metrics
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("P50 Latency", f"{stats['p50_latency']:.1f}ms",
                 delta="-75%" if stats['p50_latency'] < 200 else "+0%")

    with col2:
        st.metric("P95 Latency", f"{stats['p95_latency']:.1f}ms",
                 delta="-71%" if stats['p95_latency'] < 350 else "+0%")

    with col3:
        st.metric("Cache Hit Rate", f"{stats['cache_hit_rate']*100:.1f}%",
                 delta="+25%" if stats['cache_hit_rate'] >= 0.65 else "+0%")

    # Performance trend chart
    import plotly.graph_objects as go

    fig = go.Figure()
    fig.add_hline(y=180, line_dash="dash", line_color="green",
                  annotation_text="Target (180ms)")
    fig.add_trace(go.Scatter(
        y=tracker.metrics["response_times"][-50:],
        mode='lines',
        name='Response Time'
    ))
    fig.update_layout(title="Response Time Trend", yaxis_title="Latency (ms)")

    st.plotly_chart(fig, use_container_width=True)
```

#### Step 4.3: Set Up Alerts

```python
def check_performance_alerts(stats):
    """Alert on performance regressions."""

    if stats['p50_latency'] > 200:
        st.warning(f"‚ö†Ô∏è P50 latency above target: {stats['p50_latency']:.1f}ms > 200ms")

    if stats['cache_hit_rate'] < 0.60:
        st.warning(f"‚ö†Ô∏è Cache hit rate below 60%: {stats['cache_hit_rate']*100:.1f}%")

    if stats['p95_latency'] > 400:
        st.error(f"üö® P95 latency critical: {stats['p95_latency']:.1f}ms > 400ms")
```

---

## Rollback Plan

If issues occur, quickly rollback to original implementation:

```python
# 1. Update imports
from ghl_real_estate_ai.services.claude_assistant import ClaudeAssistant  # Original
from ghl_real_estate_ai.core.llm_client import LLMClient  # Original

# 2. Clear cache
import redis
r = redis.Redis.from_url(settings.redis_url)
r.flushdb()

# 3. Restart services
supervisorctl restart streamlit
```

---

## Testing Checklist

### Before Deployment
- [ ] Run benchmark suite: `python benchmarks/claude_performance_benchmark.py`
- [ ] Verify P50 < 200ms
- [ ] Verify cache hit rate > 60%
- [ ] Test streaming in UI (smooth, no stuttering)
- [ ] Verify connection pool stats (>90% reuse)
- [ ] Test with demo personas (Sarah, David, Maria)

### During Demo
- [ ] First query feels instant (<200ms perceived)
- [ ] Subsequent queries hit cache (<50ms)
- [ ] Streaming shows progressive responses
- [ ] No UI freezing or lag
- [ ] Cache stats show healthy hit rate

### Post-Demo
- [ ] Review performance metrics
- [ ] Check for errors in logs
- [ ] Verify connection pool health
- [ ] Monitor cache memory usage
- [ ] Collect client feedback on speed

---

## Troubleshooting

### Issue: Slow First Query
**Symptom**: First query takes 800ms+
**Solution**:
```python
# Ensure cache warming runs on startup
assistant = ClaudeAssistantOptimized()
await assistant._warm_demo_cache_background()
await asyncio.sleep(2)  # Wait for warming to complete
```

### Issue: Low Cache Hit Rate
**Symptom**: Cache hit rate < 40%
**Solution**:
```python
# Lower similarity threshold for more matches
cached = await cache.get_similar(query, threshold=0.80)  # Was 0.87

# Pre-generate more demo responses
demo_responses = [
    # Add more variations of common queries
]
```

### Issue: Connection Pool Exhaustion
**Symptom**: "Connection pool full" errors
**Solution**:
```python
# Increase pool size
client = LLMClientOptimized(max_connections=30)  # Was 20

# Or reduce keepalive time
client = LLMClientOptimized(keepalive_expiry=180)  # Was 300
```

### Issue: Streaming Stutters
**Symptom**: Jerky animation during streaming
**Solution**:
```python
# Reduce sleep time for smoother animation
await asyncio.sleep(0.005)  # Was 0.01

# Or buffer chunks
chunks = []
async for chunk in stream:
    chunks.append(chunk)
    if len(chunks) >= 3:  # Buffer 3 chunks
        response_text += ''.join(chunks)
        render(response_text)
        chunks = []
```

---

## Performance Targets Summary

| Metric | Baseline | Target | Achieved | Status |
|--------|----------|--------|----------|--------|
| P50 Response Time | 800ms | 180ms | TBD | ‚è≥ |
| P95 Response Time | 1200ms | 350ms | TBD | ‚è≥ |
| First Token Time | N/A | 180ms | TBD | ‚è≥ |
| Cache Hit Rate (Demo) | 40% | 65% | TBD | ‚è≥ |
| Connection Reuse Rate | 0% | 90% | TBD | ‚è≥ |

---

## Success Criteria

### Technical
- ‚úÖ P50 latency < 200ms
- ‚úÖ P95 latency < 400ms
- ‚úÖ First token < 200ms (streaming)
- ‚úÖ Cache hit rate > 60%
- ‚úÖ Connection reuse > 90%

### Business
- ‚úÖ Prospects comment on AI speed
- ‚úÖ No awkward waiting during demos
- ‚úÖ "Instant" AI feel impresses clients
- ‚úÖ Smooth streaming creates engagement
- ‚úÖ Zero demo failures due to latency

---

## Next Steps

1. **Review** this implementation guide
2. **Run** Phase 1 (Quick Wins) today
3. **Validate** improvements with benchmark
4. **Deploy** to demo environment
5. **Monitor** performance in real demos
6. **Iterate** based on client feedback

---

## Support

Questions or issues? Check:
- **Benchmark Results**: `benchmarks/claude_performance_benchmark.py`
- **Optimization Plan**: `CLAUDE_AI_PERFORMANCE_OPTIMIZATION_PLAN.md`
- **Code References**:
  - Optimized Assistant: `ghl_real_estate_ai/services/claude_assistant_optimized.py`
  - Optimized LLM Client: `ghl_real_estate_ai/core/llm_client_optimized.py`
  - Semantic Cache: `ghl_real_estate_ai/services/semantic_cache_optimized.py`
