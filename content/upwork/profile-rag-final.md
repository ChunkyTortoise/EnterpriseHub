# RAG Specialist Profile — READY TO PASTE

Generated: 2026-02-18

---

## Headline (62/70 chars)
```
RAG Engineer | Hybrid Search & Document AI | Python, FastAPI
```

## Overview (3187/5000 chars)
```
Most document AI systems fail in production the same way: they hallucinate answers, miss relevant chunks, or collapse when the document set scales. I build RAG pipelines engineered to avoid all three failure modes — and I have the test coverage to prove it.

My specialty is hybrid retrieval: combining BM25 keyword matching with dense semantic embeddings, then layering re-ranking on top. This approach consistently outperforms pure vector search on recall and precision, especially on domain-specific corpora where exact terminology matters (legal documents, technical manuals, real estate contracts, internal knowledge bases).

What I build for you:

- Document Q&A systems with citation tracking — every answer links back to the source chunk
- Enterprise knowledge bases with multi-format ingestion (PDF, DOCX, HTML, plain text)
- Hybrid BM25 + semantic retrieval pipelines with configurable re-ranking
- RAG evaluation frameworks — automated tests to catch retrieval regressions before they hit users
- LLM cost optimization via semantic caching (up to 89% cost reduction at 88% cache hit rate)

Technical proof:

My docqa-engine project demonstrates the full stack: BM25 + TF-IDF + semantic search with ChromaDB and FAISS backends, automated evaluation against ground-truth Q&A pairs, citation tracking to source documents, and 500+ tests covering retrieval quality under adversarial inputs. P95 latency stays under 200ms even as the corpus scales.

I have also built domain-specific RAG for real estate — a buyer/seller document assistant that processes MLS listings, disclosure packets, and contract templates to answer agent and client questions without hallucination.

Tech stack for RAG work:

LLMs: Claude API, GPT-4, Gemini | Retrieval: BM25, TF-IDF, ChromaDB, FAISS, pgvector | Backend: FastAPI (async), Python 3.11+ | Caching: Redis 3-tier | Testing: pytest, 80%+ coverage standard | Infra: Docker, GitHub Actions

Ready to see it in action?

I have a live Prompt Lab demo at https://ct-prompt-lab.streamlit.app/ and a public docqa-engine repo on GitHub. If you describe your document set and use case, I can tell you within one conversation whether your retrieval approach will hold up at scale — and what it will cost to run.
```

## Skills Tags (15)
```
1. Retrieval-Augmented Generation (RAG)
2. Large Language Models (LLM)
3. Python
4. FastAPI
5. Vector Database
6. Semantic Search
7. Natural Language Processing (NLP)
8. ChatGPT API / Claude API
9. PostgreSQL
10. AI Chatbot Development
11. Document Processing
12. Redis
13. Data Engineering
14. Machine Learning
15. Docker
```

## Character Counts
- Headline: 62 chars
- Overview: 3187 chars
