# Freelance Proposal Templates

**Author**: Cayman Roden
**Updated**: February 2026
**Usage**: Copy the template for the matching gig type, fill in [BRACKETED] placeholders with project-specific details, and send.

---

## 1. Web Scraping / Data Collection

Hi [CLIENT_NAME],

I read through your requirements for scraping [TARGET_SITE/DATA_SOURCE]. This is a well-defined project and I can deliver a production-ready scraper with structured output in [FORMAT].

I maintain 11 open-source repositories with 8,500+ automated tests. My scrapers handle anti-bot measures (Playwright/Selenium), pagination, rate limiting, and retry logic out of the box. A recent scraping pipeline I built processes 100K+ records with automatic deduplication and exports to CSV/JSON/database.

**Approach:**
- Analyze [TARGET_SITE] structure and identify optimal extraction method (API, HTML parsing, or headless browser)
- Build scraper with error handling, rate limiting, and proxy rotation if needed
- Output clean, structured data in [CSV/JSON/DB] with deduplication
- Include scheduling capability for recurring runs
- Deliver documented source code with setup instructions

**Timeline:** 1-3 days depending on site complexity.

**Pricing:** $50 (single source, static pages) / $100 (dynamic content, anti-bot handling) / $200 (multi-source pipeline with scheduling and alerts).

Send me the target URL and sample of the data you need -- I will confirm feasibility and turnaround within 2 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 2. REST API Integration

Hi [CLIENT_NAME],

Your [PLATFORM_A] to [PLATFORM_B] integration is straightforward. I have built production API integrations handling 10 req/s rate limits with 99.9% sync reliability, so this is squarely in my wheelhouse.

My main project uses FastAPI with full OAuth2/JWT authentication, Pydantic validation on every endpoint, and automated test suites covering edge cases. I have integrated GoHighLevel, Stripe, and custom webhook systems in production with real-time data sync.

**Approach:**
- Map the data flow between [PLATFORM_A] and [PLATFORM_B] APIs
- Build the integration layer with authentication, error handling, and retry logic
- Add webhook listeners or polling (whichever fits your use case)
- Write tests covering auth failures, rate limits, and data validation edge cases
- Deploy with monitoring and alerting for failed syncs

**Timeline:** 2-5 days depending on API complexity and number of endpoints.

**Pricing:** $75 (single endpoint, one-directional sync) / $150 (multi-endpoint, bidirectional) / $300 (full integration suite with monitoring, error recovery, and dashboard).

Happy to jump on a quick call to map out the data flow, or send me your API docs and I will spec the integration same-day.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 3. Data Dashboard (Streamlit/Plotly)

Hi [CLIENT_NAME],

Turning [YOUR_DATA_TYPE] into an interactive dashboard is exactly what I do. I have built BI dashboards with 313 automated tests that load 100K rows in under 500ms with auto-detected chart types at 91% accuracy.

My production dashboards include Monte Carlo simulations, sentiment analysis, churn detection, and KPI tracking -- all in Streamlit with Plotly visualizations. Every dashboard I deliver handles messy data (missing values, duplicates, inconsistent formatting) automatically.

**Approach:**
- Ingest your [CSV/EXCEL/DATABASE] data and auto-clean it
- Build [NUMBER] interactive charts with filters, drill-downs, and tooltips
- Add KPI summary panel with trend indicators and sparklines
- Match your brand colors, logo, and fonts
- Export options: PNG, PDF, interactive HTML, or hosted Streamlit URL

**Timeline:** 3-5 days for a full multi-page dashboard.

**Pricing:** $50 (3 charts, single data source, PNG export) / $125 (8 charts, multi-source, PDF export, KPI panel) / $200 (15+ charts, ML forecasting, anomaly detection, live data API).

Send me your data file (CSV or Excel) and the 3 metrics that matter most to you -- I will reply with a free preview chart within 24 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 4. AI Chatbot Development

Hi [CLIENT_NAME],

Your [USE_CASE -- e.g., lead qualification / customer support / FAQ] chatbot is a strong fit for my multi-agent architecture. I have 4,937 automated tests covering conversation flow, handoff precision, and CRM sync, with a 94% handoff success rate in production.

My chatbot system runs 3 production bot personas with <200ms response time, real-time CRM integration (GoHighLevel, HubSpot, Salesforce), and automatic lead scoring. The handoff engine transfers conversations between specialist bots at 0.7 confidence with full context preservation -- no conversation dropped, no customer bounced in circles.

**Approach:**
- Define bot persona, tone, and intent map for [YOUR_INDUSTRY]
- Build conversation flows with [NUMBER] intents and fallback handling
- Integrate with [YOUR_CRM] for contact creation, lead scoring, and hot/warm/cold tagging
- Add analytics: message volume, popular queries, conversion tracking
- Deploy as embeddable web widget or REST API

**Timeline:** 5-10 days depending on number of personas and integrations.

**Pricing:** $150 (single bot, 20+ intents, web widget) / $350 (3 bot personas, CRM sync, lead scoring, handoff) / $500 (5+ agents, A/B testing, analytics dashboard, alerting, SLA tools).

Message me your website URL and use case -- I will spec out a custom chatbot architecture for free, no obligation.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 5. RAG Document Q&A System

Hi [CLIENT_NAME],

Building a [DOCUMENT_TYPE] Q&A system is my strongest niche. I built a production RAG pipeline with 94% retrieval precision, 96% citation accuracy, and a 0.89 RAGAS evaluation score -- backed by 322 automated tests.

My RAG system uses hybrid search (dense + sparse retrieval), reranking pipelines, and L1/L2/L3 caching that delivers answers in under 200ms. It handles multi-document queries across hundreds of files simultaneously, with every answer linked to the exact source passage and page number. No hallucination risk -- every response is grounded in your documents.

**Approach:**
- Ingest your [DOCUMENT_TYPE] files with optimized chunking for your content structure
- Configure hybrid retrieval (keyword + semantic) tuned to your domain vocabulary
- Build web interface with source citations, highlighted passages, and follow-up questions
- Add admin dashboard for monitoring query patterns and retrieval quality
- Deploy via Docker with scaling capabilities

**Timeline:** 3-7 days depending on document volume and customization.

**Pricing:** $100 (single document, web UI, citations) / $250 (100 docs, hybrid search, branded UI, conversation history) / $500 (unlimited docs, RAGAS evaluation dashboard, REST API, multi-user, fine-tuned embeddings).

Send me a sample PDF and 3 example questions your team would ask -- I will return a working demo within 48 hours, free, no obligation.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 6. Python Automation Script

Hi [CLIENT_NAME],

Your [TASK_DESCRIPTION] automation is a clean build. I write Python automation backed by 8,500+ tests across 11 production repositories, so reliability is baked into everything I deliver.

A recent automation pipeline I built reduced a client's manual data processing from 4 hours to 12 minutes with zero errors. My scripts include proper error handling, logging, retry logic, and documentation -- not throwaway code that breaks on the first edge case.

**Approach:**
- Analyze your current manual workflow for [TASK_DESCRIPTION]
- Build Python script with error handling, logging, and input validation
- Add scheduling (cron/Task Scheduler) if you need recurring execution
- Include email/Slack notifications for success/failure
- Deliver documented source code with a setup guide and requirements.txt

**Timeline:** 1-2 days for most automation tasks.

**Pricing:** $25 (single-task script, basic error handling) / $50 (multi-step workflow, scheduling, notifications) / $100 (full pipeline with monitoring, retry logic, and deployment guide).

Send me a description of the manual steps you want automated and I will confirm scope and turnaround within 2 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 7. Data Cleaning / ETL Pipeline

Hi [CLIENT_NAME],

Cleaning [DATA_DESCRIPTION] and building a reliable pipeline is straightforward work for me. My production systems handle 100K+ row datasets with automatic deduplication, type conversion, and validation -- all tested with comprehensive suites.

I recently built an ETL pipeline that merges data from 5 sources, handles missing values and inconsistent formatting, and loads clean output into PostgreSQL with full audit logging. Data cleaning is included at no extra charge in every project I deliver.

**Approach:**
- Profile your data: row counts, column types, missing values, duplicates, outliers
- Build cleaning pipeline: deduplication, type normalization, null handling, format standardization
- Transform and merge from [SOURCE_A], [SOURCE_B] into [TARGET_FORMAT/DATABASE]
- Add validation checks that flag anomalies before loading
- Deliver reusable pipeline with scheduling for recurring data loads

**Timeline:** 1-3 days depending on data volume and source complexity.

**Pricing:** $25 (single file cleanup, CSV/Excel output) / $75 (multi-source merge, database load, validation) / $150 (full ETL pipeline with scheduling, monitoring, and error alerts).

Send me a sample of your data (or describe the columns and row count) and I will reply with a data profile report and a fixed quote within 4 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 8. Streamlit Web App

Hi [CLIENT_NAME],

Building [APP_DESCRIPTION] as a Streamlit app is a great fit. I have deployed production Streamlit applications with multi-page navigation, real-time data connections, role-based access, and sub-500ms load times across 100K+ rows.

My Streamlit portfolio includes BI dashboards with Monte Carlo simulations, sentiment analysis panels, and full CRUD interfaces backed by PostgreSQL. Every app I deliver is responsive (desktop, tablet, mobile), documented, and deployment-ready.

**Approach:**
- Design the app layout: pages, sidebar navigation, user flows
- Build core functionality for [APP_DESCRIPTION] with input validation and error handling
- Connect to your data source ([DATABASE/API/FILE])
- Add authentication if needed (basic auth, SSO, or custom)
- Deploy to Streamlit Cloud, Railway, or Docker -- your choice

**Timeline:** 3-7 days depending on complexity and number of pages.

**Pricing:** $50 (single-page app, basic inputs/outputs) / $125 (multi-page, database connection, file uploads, sidebar filters) / $250 (full application with auth, API integrations, deployment, and admin panel).

Describe the app you need and the data it works with -- I will send you a wireframe and scope estimate within 24 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 9. FastAPI Backend Development

Hi [CLIENT_NAME],

Your [API_DESCRIPTION] backend is a direct match for my stack. My primary production system is FastAPI with async endpoints, JWT authentication, Pydantic validation, PostgreSQL + Alembic migrations, and Redis caching -- all running at <200ms response overhead.

I have built APIs handling real-time CRM sync at 10 req/s with 99.9% reliability, complete with rate limiting, L1/L2/L3 caching (88% cache hit rate), and full OpenAPI documentation auto-generated from the code. Every endpoint is tested -- 8,500+ tests across my repositories.

**Approach:**
- Design the API schema: endpoints, request/response models, auth flow
- Build FastAPI backend with async handlers, Pydantic models, and auto-generated docs
- Set up PostgreSQL with Alembic migrations (or your preferred database)
- Add authentication (JWT/OAuth2), rate limiting, and input validation
- Containerize with Docker and provide deployment instructions

**Timeline:** 3-7 days depending on endpoint count and integration complexity.

**Pricing:** $75 (3-5 endpoints, basic auth, SQLite) / $200 (10+ endpoints, JWT, PostgreSQL, Redis cache, Docker) / $500 (full backend with async workers, WebSocket support, monitoring, CI/CD pipeline).

Send me your API requirements (even rough notes work) and I will return an OpenAPI spec draft within 24 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---

## 10. LLM/AI Integration (Claude/GPT API)

Hi [CLIENT_NAME],

Integrating [LLM_PROVIDER] into [YOUR_APPLICATION] is core to what I build daily. My production AI orchestration system achieved an 89% LLM cost reduction through multi-strategy parsing, L1/L2/L3 caching (88% cache hit rate), and intelligent model routing -- all at <200ms overhead.

I work with Claude, GPT-4, and Gemini APIs in production, with automatic fallback chains, token optimization, and structured output parsing. My orchestrator handles prompt management, conversation memory, function calling, and cost tracking across multiple models simultaneously.

**Approach:**
- Define your AI use case: [TASK -- e.g., content generation, classification, extraction, conversation]
- Build the integration layer with prompt engineering, structured output parsing, and error handling
- Add caching to eliminate redundant API calls (typically 70-89% cost reduction)
- Implement model fallback chain (primary model -> backup -> local) for reliability
- Include cost monitoring dashboard and token usage tracking

**Timeline:** 2-5 days depending on complexity and number of AI features.

**Pricing:** $100 (single LLM integration, basic prompt engineering, structured output) / $250 (multi-model orchestration, caching, fallback chain, cost tracking) / $500 (full AI system with conversation memory, function calling, A/B prompt testing, analytics dashboard).

Send me your use case and expected input/output format -- I will prototype a working integration and send you sample outputs within 48 hours.

Best,
Cayman Roden
Portfolio: https://chunkytortoise.github.io
GitHub: https://github.com/ChunkyTortoise

---
---

## QUICK RESPONSE TEMPLATE

**Use for**: Simple gig postings, Fiverr Buyer Requests, or rapid-fire responses where the client described the job clearly. Under 100 words.

---

Hi [CLIENT_NAME],

I can deliver [SPECIFIC_DELIVERABLE] within [TIMEFRAME]. I have built [RELEVANT_SYSTEM] in production with [KEY_METRIC] -- this is a straightforward build for me.

**What I need from you:** [1-2 specific items, e.g., "your CSV file and target output format"].

I will start immediately upon order. Check my GitHub (github.com/ChunkyTortoise) for code quality -- 11 repos, 8,500+ tests, all CI green.

Ready when you are.

-- Cayman

---
---

## USAGE NOTES

### Filling Placeholders
- Replace all `[BRACKETED]` text with project-specific details pulled from the client's posting
- The more specific your replacements, the higher the conversion rate
- If the client mentioned a specific technology, tool, or pain point -- reference it in the opening line

### Pricing Adjustments
- **Review-building phase** (0-5 reviews): Use the lower end of each range
- **Established phase** (5+ reviews, 4.8+ rating): Use mid-to-upper range
- **Premium phase** (15+ reviews): Add 50-100% to all tiers
- Always quote a range, not a fixed number -- it opens negotiation upward

### Response Speed
- Respond within 15 minutes on Fiverr (algorithm rewards fast responders)
- On Upwork, respond within 1 hour
- For email outreach, same-day response to any reply

### Upsell Trigger
After every delivery, include: "I noticed [OBSERVATION_ABOUT_THEIR_DATA/SYSTEM]. I can add [SPECIFIC_UPGRADE] for $[PRICE] -- want me to scope it out?"
