# Cold Outreach Batch 2 -- FORMATTED FOR SENDING

**Date**: February 25, 2026 (scheduled send)
**Status**: READY-TO-SEND-HUMAN -- Copy and send from Gmail. Do NOT automate.

---

Live Demo URLs (all confirmed):
- Prompt Lab: https://ct-prompt-lab.streamlit.app/
- LLM Starter: https://ct-llm-starter.streamlit.app/
- AgentForge: https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/
- Portfolio: https://chunkytortoise.github.io

---

## Send Order (ranked by score)

| Rank | Company | Score | Contact | Channel |
|------|---------|-------|---------|---------|
| 1 | Guru | 7.8 | Rick Nucci, CEO | LinkedIn DM / guru.com contact |
| 2 | Ada | 7.5 | Mike Murchison, CEO | LinkedIn DM / ada.cx contact |
| 3 | Lofty (Chime) | 7.2 | Joe Chen, CEO | LinkedIn DM / lofty.com contact |
| 4 | Movable Ink | 7.0 | Vivek Sharma, CEO | LinkedIn DM / movableink.com contact |
| 5 | Real Geeks | 6.8 | Kevin McCarthy, CEO | LinkedIn DM / realgeeks.com contact |
| 6 | Assembled | 6.5 | Ryan Wang, CEO | LinkedIn DM / assembled.com contact |
| 7 | Crexi | 6.3 | Michael DeGiorgio, CEO | LinkedIn DM / crexi.com contact |
| 8 | Rechat | 6.0 | Shayan Hamidi, CEO | LinkedIn DM / rechat.com contact |
| 9 | Streamline Results | 5.8 | Jonathan Johnson, Founder | LinkedIn DM / streamlineresults.com contact |
| 10 | Dashworks | 5.5 | Ivan Zhou, Founder | LinkedIn DM / dashworks.ai contact |

---

## 1. Rick Nucci -- Guru

**To**: Rick Nucci (LinkedIn DM or guru.com contact)
**Subject**: Guru's knowledge queries are 88% cacheable -- here's the architecture

Hi Rick,

Congrats on Guru turning 10 and crossing $63M in revenue. Building an "AI Source of Truth" that enterprises actually trust is rare.

Here is what caught my attention: knowledge management queries are among the most cacheable AI workloads. "How does our expense policy work?" gets asked hundreds of times with slight variations across your customer base. I built a system that exploits exactly this pattern:

- **88% cache hit rate** via 3-tier caching (L1 in-memory, L2 Redis, L3 semantic)
- **89% LLM cost reduction** -- at $63M revenue, even 30% savings on AI ops is material
- **Citation faithfulness: 0.88** -- every answer traceable to source documents

With your Boomi background, I think you would appreciate the architectural rigor: the semantic cache layer detects that "What's our PTO policy?" and "How many vacation days do I get?" are the same query and serves the cached result.

Live demo: https://ct-prompt-lab.streamlit.app/

Worth a 15-minute walkthrough?

Cayman Roden
Python/AI Engineer | Fractional AI CTO
caymanroden@gmail.com | (310) 982-0492

---

## 2. Mike Murchison -- Ada

**To**: Mike Murchison (LinkedIn DM or ada.cx contact)
**Subject**: Ada's Reasoning Engine + 3-tier caching = 89% cost reduction

Hi Mike,

Ada's Reasoning Engine is solving one of the hardest problems in customer service AI -- an agent that resolves, answers, AND upsells. Most chatbots struggle with just one of those.

At unicorn scale ($1B+ valuation, $200M raised), LLM cost efficiency directly impacts the unit economics investors scrutinize. Customer service queries are among the most cacheable AI workloads -- "Where's my order?" gets asked millions of times with minor variations.

I built a multi-agent orchestration system with results directly relevant to Ada:

- **88% cache hit rate** via semantic caching that catches paraphrased queries
- **89% LLM cost reduction** -- moves the needle on unit economics
- **Confidence-scored handoffs** between specialized agents with zero context loss

Even a 20-30% improvement on LLM costs at Ada's scale saves millions annually.

Live demo: https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/

Worth a 30-minute discovery call?

Cayman Roden
Python/AI Engineer | Fractional AI CTO
caymanroden@gmail.com | (310) 982-0492

---

## 3. Joe Chen -- Lofty (Chime)

**To**: Joe Chen (LinkedIn DM or lofty.com contact)
**Subject**: Lofty's AI Workforce -- the infrastructure layer that makes it scale

Hi Joe,

The rebrand from Chime to Lofty makes strategic sense. The AI Workforce concept -- multi-agent, predictive, always-on -- is exactly where real estate tech is heading.

I built the infrastructure layer that makes multi-agent real estate AI actually work at scale. Three specialized agents (Lead, Buyer, Seller) coordinated through an orchestration engine with:

- **3-tier caching** that reduced LLM costs by 89% (88% cache hit rate)
- **Sub-200ms orchestration** for multi-agent coordination
- **Learned handoff thresholds** that improve accuracy over time from historical outcomes

For Lofty's AI Workforce, this infrastructure means faster responses, lower per-conversation costs, and smarter agent-to-agent handoffs as your platform scales across thousands of agents.

Live demo: https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/

Open to a 15-minute call to walk through the architecture?

Cayman Roden
Python/AI Engineer | Multi-Agent AI Systems
caymanroden@gmail.com | (310) 982-0492

---

## 4. Vivek Sharma -- Movable Ink

**To**: Vivek Sharma (LinkedIn DM or movableink.com contact)
**Subject**: Autonomous Marketing AI -- cutting LLM costs under PE ownership

Hi Vivek,

Congrats on the STG partnership. The Autonomous Marketing Capabilities launch is the right move -- AI agents for personalization at scale is where marketing is heading.

Under PE ownership, AI cost efficiency becomes a board-level metric. Marketing personalization generates enormous LLM volume, and most queries follow repeatable patterns across similar customer segments. I built a multi-agent system that exploits this:

- **4.3M agent dispatches/sec** throughput on the orchestration engine
- **88% cache hit rate** -- similar customer segments get cached responses
- **89% LLM cost reduction** ($3,600/mo to $400/mo on one deployment)

For Movable Ink's scale, even a 30-40% optimization on the AI layer materially improves margins -- exactly what STG's portfolio model requires.

Live demo: https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/

Worth a 30-minute discovery call?

Cayman Roden
Python/AI Engineer | Fractional AI CTO
caymanroden@gmail.com | (310) 982-0492

---

## 5. Kevin McCarthy -- Real Geeks

**To**: Kevin McCarthy (LinkedIn DM or realgeeks.com contact)
**Subject**: Geek AI has been running for years -- time for an architecture refresh?

Hi Kevin,

Real Geeks was one of the first real estate CRMs to adopt AI. That early investment compounds -- but after years in production, the architecture likely has room for a performance uplift.

I built a modern AI orchestration layer for real estate that addresses the common pain points of mature deployments:

- **89% LLM cost reduction** through 3-tier caching (L1 in-memory, L2 Redis, L3 semantic)
- **Multi-LLM orchestration** with automatic fallback chains -- no single-model dependency
- **P50/P95/P99 latency tracking** with configurable alert rules

With your CS background, I think you would appreciate the engineering rigor: 8,500+ automated tests across 11 production repos, and every system benchmarked at P50/P95/P99.

Live demo: https://ct-prompt-lab.streamlit.app/

Worth a 15-minute look at the architecture?

Cayman Roden
Python/AI Engineer | Production AI Systems
caymanroden@gmail.com | (310) 982-0492

---

## 6. Ryan Wang -- Assembled

**To**: Ryan Wang (LinkedIn DM or assembled.com contact)
**Subject**: AI-to-human handoffs -- zero context loss at P99 0.095ms

Hi Ryan,

Assembled's positioning on managing both human AND AI agents is exactly right. The next challenge for support ops is orchestrating the handoffs between them without losing context.

I built a multi-agent system that handles this with Stripe-level engineering rigor:

- **Confidence-scored handoffs** -- AI routes to human (or other AI) based on thresholds
- **Zero context loss** -- conversation history, qualification data, and intent signals transfer completely
- **P99: 0.095ms** coordination latency between agents
- **8,500+ automated tests** across 11 production repos

With your Stripe background, you know what production-grade means. The system preserves full context during handoff using enriched context objects -- so human agents pick up where the AI left off, not from scratch.

Live demo: https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/

Worth a 15-minute walkthrough?

Cayman Roden
Python/AI Engineer | Fractional AI CTO
caymanroden@gmail.com | (310) 982-0492

---

## 7. Michael DeGiorgio -- Crexi

**To**: Michael DeGiorgio (LinkedIn DM or crexi.com contact)
**Subject**: $1T in CRE listings -- RAG pipeline architecture for that scale

Hi Michael,

Congrats on Crexi surpassing $1 trillion in listed property value. That data moat is formidable.

With 500,000+ commercial listings, your AI insights pipeline processes an enormous document corpus. I built a RAG system designed for exactly this kind of high-volume real estate data:

- **Hybrid BM25 + semantic search + re-ranking** for accurate retrieval across large corpora
- **89% LLM cost reduction** via 3-tier caching (CRE queries follow regional and property-type patterns)
- **Citation faithfulness: 0.88** -- every AI insight traceable to source listings

For Crexi's data scale, semantic caching detects that "retail space in downtown LA" and "commercial retail, DTLA" are the same query and serves the cached result.

Live demo: https://ct-prompt-lab.streamlit.app/

Worth a 30-minute discovery call to discuss the architecture?

Cayman Roden
Python/AI Engineer | RAG & AI Systems
caymanroden@gmail.com | (310) 982-0492

---

## 8. Shayan Hamidi -- Rechat

**To**: Shayan Hamidi (LinkedIn DM or rechat.com contact)
**Subject**: Fractional AI CTO for Rechat -- production-grade AI without the $250K hire

Hi Shayan,

Rechat's AI-powered agent matching tool is a smart extension of the platform. Matching agents using listing data and location filters is a scoring problem I have solved at scale.

For a 39-person team building AI features, the gap between "works in demo" and "works in production" is where most teams burn 3-6 months. I bridge that gap as a fractional AI CTO:

- **11 production repos** with 8,500+ automated tests
- **89% LLM cost reduction** via 3-tier caching -- critical when watching margins at $2.2M revenue
- **Multi-LLM orchestration** (Claude + Gemini) with automatic fallback chains

My entry point is a $2,500 Architecture Audit -- a scored assessment across six categories with P50/P95/P99 benchmarks and a migration roadmap. Most audits identify $5K-$15K/month in savings.

Live demo: https://ai-orchest-7mnwp9untg7gyyvchzevid.streamlit.app/

Would a 15-minute demo of the architecture be useful?

Cayman Roden
Python/AI Engineer | Fractional AI CTO
caymanroden@gmail.com | (310) 982-0492

---

## 9. Jonathan Johnson -- Streamline Results

**To**: Jonathan Johnson (LinkedIn DM or streamlineresults.com contact)
**Subject**: GHL Certified Partner + AI orchestration = premium service tier

Hi Jonathan,

Respect the Certified GHL Partner status. Most agencies treat GHL as a CRM -- you treat it as infrastructure. That is a different level.

Quick question: are your local business clients getting sub-minute AI responses to new leads? Most GHL setups I audit are either too slow (generic autoresponders) or too expensive (every message hitting the LLM API directly).

I built a system that does both -- fast AND cheap:

- **89% LLM cost reduction** via 3-tier Redis caching (88% hit rate)
- **Sub-200ms orchestration** for bot-to-bot handoffs with zero context loss
- **Hot/Warm/Cold tagging** that triggers the right workflow automatically

For a certified partner, this becomes a premium service tier. Your local business clients get smarter leads, faster responses, and lower costs.

Live demo: https://ct-llm-starter.streamlit.app/

Worth a free 15-minute audit of one of your GHL setups?

Cayman Roden
Python/AI Engineer | GHL + AI Specialist
caymanroden@gmail.com | (310) 982-0492

---

## 10. Ivan Zhou -- Dashworks

**To**: Ivan Zhou (LinkedIn DM or dashworks.ai contact)
**Subject**: Multi-source RAG at 88% cache hit rate -- YC startup cost math

Hi Ivan,

Dashworks' approach to connecting company knowledge across Slack, Notion, Drive, and Jira into a single AI assistant is the right product at the right time. Knowledge fragmentation is universal.

Multi-source RAG is one of the hardest problems in AI engineering. I have built production systems that handle similar complexity:

- **BM25 + semantic search + re-ranking** across multiple data sources
- **88% cache hit rate** -- knowledge queries are highly repetitive ("How does our deployment process work?" gets asked constantly)
- **89% LLM cost reduction** -- critical for a YC startup managing burn rate
- **Citation faithfulness: 0.88** -- answers traced back to source documents

At the early stage, a fractional AI architecture advisor accelerates development without the cost of a senior hire. My $2,500 Architecture Audit identifies the highest-ROI improvements in your RAG pipeline.

Live demo: https://ct-prompt-lab.streamlit.app/

Worth a 30-minute discovery call?

Cayman Roden
Python/AI Engineer | Fractional AI CTO
caymanroden@gmail.com | (310) 982-0492

---

## Post-Send Checklist

- [ ] Send LinkedIn connection requests same evening as each email
- [ ] Track opens/clicks for each email
- [ ] Prepare technical brief PDF for "send it" replies
- [ ] Have Calendly link ready for discovery call bookings
- [ ] Day 3 follow-up: Send Email 2 for all 10
- [ ] Day 7 follow-up: Send Email 3 final touches for all 10
- [ ] Non-responders after Email 3: Move to nurture sequence
- [ ] Update CAMPAIGN_TRACKER_V2.md with send times
- [ ] Monitor replies within 4 hours during business hours
