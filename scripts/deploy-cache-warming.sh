#!/bin/bash

# Intelligent Cache Warming Service Production Deployment Script
# Deploys ML-powered cache warming service to AWS EKS with Redis cluster integration
# Version: 2.0.0

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
DEPLOYMENT_ENV="${DEPLOYMENT_ENV:-production}"
AWS_REGION="${AWS_REGION:-us-east-1}"
EKS_CLUSTER_NAME="${EKS_CLUSTER_NAME:-jorge-platform-eks}"
IMAGE_REGISTRY="${IMAGE_REGISTRY:-your-account.dkr.ecr.us-east-1.amazonaws.com}"
IMAGE_NAME="jorge-platform/intelligent-cache-warming"
IMAGE_TAG="${IMAGE_TAG:-v2.0.0}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Error handler
error_exit() {
    log_error "$1"
    exit 1
}

# Check prerequisites
check_prerequisites() {
    log_info "Checking prerequisites for cache warming deployment..."

    # Check required commands
    command -v aws >/dev/null 2>&1 || error_exit "AWS CLI not found"
    command -v kubectl >/dev/null 2>&1 || error_exit "kubectl not found"
    command -v docker >/dev/null 2>&1 || error_exit "Docker not found"

    # Check AWS authentication
    aws sts get-caller-identity >/dev/null 2>&1 || error_exit "AWS authentication failed"

    # Check kubectl context
    kubectl config current-context >/dev/null 2>&1 || error_exit "kubectl context not set"

    # Check if Redis cluster is accessible
    log_info "Checking Redis cluster connectivity..."
    # This would be a real Redis connectivity check in production

    log_success "Prerequisites check passed"
}

# Build and push cache warming Docker image
build_and_push_image() {
    log_info "Building intelligent cache warming Docker image..."

    cd "$PROJECT_ROOT"

    # Create Dockerfile for cache warming service
    cat > Dockerfile.cache-warming << 'EOF'
# Intelligent Cache Warming Service Production Dockerfile
FROM python:3.11-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create application user
RUN groupadd -r jorge && useradd -r -g jorge -d /app -s /sbin/nologin jorge

# Create application directory
WORKDIR /app
RUN chown jorge:jorge /app

# Install Python dependencies
COPY requirements-production.txt /app/
RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir -r requirements-production.txt

# Copy application code
COPY --chown=jorge:jorge ghl_real_estate_ai/services/intelligent_cache_warming_service.py /app/
COPY --chown=jorge:jorge ghl_real_estate_ai/services/cache_service.py /app/
COPY --chown=jorge:jorge ghl_real_estate_ai/ghl_utils/ /app/ghl_utils/
COPY --chown=jorge:jorge production_cache_warming_server.py /app/
COPY --chown=jorge:jorge pattern_analysis_job.py /app/

# Create configuration
RUN cat > /app/config.yaml << 'EOFCONFIG'
server:
  host: 0.0.0.0
  port: 8080
  workers: 1

cache_warming:
  pattern_analysis_interval: 15  # minutes
  warming_execution_interval: 5   # minutes
  prediction_window_minutes: 30
  confidence_threshold: 0.7
  max_warming_tasks: 100
  warming_concurrency: 5

redis:
  cluster_mode: true
  connection_pool_size: 20
  max_connections: 100
  timeout_seconds: 5

monitoring:
  metrics_enabled: true
  health_check_interval: 10
EOFCONFIG

# Health check script
RUN cat > /app/health_check.py << 'EOFHEALTH'
#!/usr/bin/env python3
import asyncio
import aiohttp
import sys
import time

async def health_check():
    try:
        timeout = aiohttp.ClientTimeout(total=5)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get('http://localhost:8080/health') as response:
                if response.status == 200:
                    print("Health check passed")
                    sys.exit(0)
                else:
                    print(f"Health check failed: HTTP {response.status}")
                    sys.exit(1)
    except Exception as e:
        print(f"Health check failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(health_check())
EOFHEALTH

RUN chmod +x /app/health_check.py

# Switch to non-root user
USER jorge

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python3 /app/health_check.py

# Labels
LABEL maintainer="jorge-platform-team@example.com"
LABEL version="2.0.0"
LABEL component="intelligent-cache-warming"
LABEL performance.target=">95% hit rate"

# Default command
CMD ["python3", "production_cache_warming_server.py"]
EOF

    # Login to ECR
    aws ecr get-login-password --region "$AWS_REGION" | docker login --username AWS --password-stdin "$IMAGE_REGISTRY"

    # Create ECR repository if it doesn't exist
    aws ecr describe-repositories --repository-names "${IMAGE_NAME}" --region "$AWS_REGION" >/dev/null 2>&1 || {
        log_info "Creating ECR repository: ${IMAGE_NAME}"
        aws ecr create-repository --repository-name "${IMAGE_NAME}" --region "$AWS_REGION"
    }

    # Build image
    log_info "Building Docker image for cache warming service..."
    docker build \
        -f Dockerfile.cache-warming \
        -t "${IMAGE_REGISTRY}/${IMAGE_NAME}:${IMAGE_TAG}" \
        -t "${IMAGE_REGISTRY}/${IMAGE_NAME}:latest" \
        .

    # Push to registry
    log_info "Pushing image to ECR..."
    docker push "${IMAGE_REGISTRY}/${IMAGE_NAME}:${IMAGE_TAG}"
    docker push "${IMAGE_REGISTRY}/${IMAGE_NAME}:latest"

    # Cleanup
    rm -f Dockerfile.cache-warming

    log_success "Cache warming Docker image built and pushed successfully"
}

# Configure kubectl for EKS
configure_kubectl() {
    log_info "Configuring kubectl for EKS cluster..."

    aws eks update-kubeconfig \
        --region "$AWS_REGION" \
        --name "$EKS_CLUSTER_NAME"

    # Verify connection
    kubectl get nodes >/dev/null 2>&1 || error_exit "Failed to connect to EKS cluster"

    log_success "kubectl configured for EKS cluster"
}

# Deploy Kubernetes resources
deploy_kubernetes() {
    log_info "Deploying intelligent cache warming service to Kubernetes..."

    cd "$PROJECT_ROOT"

    # Update deployment with current configuration
    cp infrastructure/kubernetes/intelligent-cache-warming-deployment.yaml /tmp/cache-warming-deployment.yaml

    # Replace placeholders in deployment file
    sed -i.bak "s|jorge-platform/intelligent-cache-warming:v2.0.0|${IMAGE_REGISTRY}/${IMAGE_NAME}:${IMAGE_TAG}|g" /tmp/cache-warming-deployment.yaml

    # Apply Kubernetes resources
    log_info "Applying Kubernetes manifests..."
    kubectl apply -f /tmp/cache-warming-deployment.yaml

    # Wait for deployment to be ready
    log_info "Waiting for cache warming deployment to be ready..."
    kubectl wait --for=condition=available --timeout=300s deployment/intelligent-cache-warming -n jorge-platform

    log_success "Cache warming Kubernetes deployment completed"
}

# Setup Redis integration
setup_redis_integration() {
    log_info "Setting up Redis cluster integration..."

    # Create Redis configuration secret
    kubectl create secret generic cache-warming-redis-config \
        --from-literal=redis_host="${REDIS_ENDPOINT:-jorge-redis-cluster.cache.amazonaws.com}" \
        --from-literal=redis_port="6379" \
        --from-literal=redis_password="${REDIS_PASSWORD:-}" \
        -n jorge-platform \
        --dry-run=client -o yaml | kubectl apply -f -

    # Test Redis connectivity from a cache warming pod
    log_info "Testing Redis connectivity..."
    CACHE_WARMING_POD=$(kubectl get pods -n jorge-platform -l app=intelligent-cache-warming -o jsonpath='{.items[0].metadata.name}')

    if [ -n "$CACHE_WARMING_POD" ]; then
        kubectl exec -n jorge-platform "$CACHE_WARMING_POD" -- python3 -c "
import redis
import os
try:
    r = redis.Redis(
        host=os.getenv('REDIS_CLUSTER_HOST', 'jorge-redis-cluster.cache.amazonaws.com'),
        port=int(os.getenv('REDIS_CLUSTER_PORT', '6379')),
        password=os.getenv('REDIS_PASSWORD'),
        socket_timeout=5
    )
    r.ping()
    print('Redis connectivity: SUCCESS')
except Exception as e:
    print(f'Redis connectivity: FAILED - {e}')
    exit(1)
"
    else
        log_warning "No cache warming pods found for Redis connectivity test"
    fi

    log_success "Redis integration setup completed"
}

# Setup monitoring
setup_monitoring() {
    log_info "Setting up cache warming monitoring..."

    # Apply Prometheus monitoring rules
    kubectl apply -f - << 'EOF'
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cache-warming-business-rules
  namespace: jorge-platform
  labels:
    app: intelligent-cache-warming
spec:
  groups:
  - name: cache.warming.business.rules
    interval: 30s
    rules:
    # Business impact metrics
    - record: jorge:cache_hit_rate_improvement
      expr: |
        (
          rate(cache_warming_hits_total[5m]) /
          (rate(cache_warming_hits_total[5m]) + rate(cache_warming_misses_total[5m]))
        ) - 0.70  # Baseline 70% hit rate

    - record: jorge:response_time_improvement_percent
      expr: |
        (
          (jorge_ml_request_duration_seconds{quantile="0.95"} -
           on() jorge_baseline_response_time_seconds) /
          on() jorge_baseline_response_time_seconds
        ) * -100

    - record: jorge:cache_warming_business_impact_score
      expr: |
        (jorge:cache_hit_rate_improvement * 0.6) +
        (jorge:response_time_improvement_percent * 0.4)
EOF

    # Create Grafana dashboard ConfigMap
    kubectl create configmap cache-warming-dashboard \
        --from-literal=dashboard.json='{
          "dashboard": {
            "title": "Intelligent Cache Warming - Business Intelligence",
            "panels": [
              {
                "title": "Cache Hit Rate Improvement",
                "type": "stat",
                "targets": [
                  {
                    "expr": "jorge:cache_hit_rate_improvement * 100",
                    "legendFormat": "Hit Rate Improvement %"
                  }
                ]
              },
              {
                "title": "Response Time Improvement",
                "type": "stat",
                "targets": [
                  {
                    "expr": "jorge:response_time_improvement_percent",
                    "legendFormat": "Response Time Improvement %"
                  }
                ]
              },
              {
                "title": "Cache Warming Queue Health",
                "type": "graph",
                "targets": [
                  {
                    "expr": "cache_warming_queue_size",
                    "legendFormat": "Queue Size"
                  },
                  {
                    "expr": "cache_warming_active_tasks",
                    "legendFormat": "Active Tasks"
                  }
                ]
              }
            ]
          }
        }' \
        -n monitoring \
        --dry-run=client -o yaml | kubectl apply -f -

    log_success "Cache warming monitoring setup completed"
}

# Run validation tests
run_validation_tests() {
    log_info "Running cache warming validation tests..."

    # Port forward for testing
    kubectl port-forward -n jorge-platform svc/intelligent-cache-warming-service 8080:80 &
    PF_PID=$!
    sleep 5

    # Test health endpoint
    HEALTH_RESPONSE=$(curl -s http://localhost:8080/health || echo "failed")
    if [[ "$HEALTH_RESPONSE" == *"healthy"* ]]; then
        log_success "Health check: PASSED"
    else
        log_warning "Health check: FAILED or INCONCLUSIVE"
        echo "Response: $HEALTH_RESPONSE"
    fi

    # Test stats endpoint
    STATS_RESPONSE=$(curl -s http://localhost:8080/stats || echo "failed")
    if [[ "$STATS_RESPONSE" == *"total_warmed"* ]]; then
        log_success "Stats endpoint: PASSED"
    else
        log_warning "Stats endpoint: FAILED"
    fi

    # Test warming endpoint
    curl -s -X POST \
        -H "Content-Type: application/json" \
        -d '{"cache_keys":["test_key_1","test_key_2"],"priority":"high"}' \
        http://localhost:8080/warm >/dev/null

    if [ $? -eq 0 ]; then
        log_success "Cache warming API: PASSED"
    else
        log_warning "Cache warming API: FAILED"
    fi

    # Clean up
    kill $PF_PID 2>/dev/null || true

    # Check logs for any errors
    log_info "Checking cache warming service logs..."
    CACHE_WARMING_POD=$(kubectl get pods -n jorge-platform -l app=intelligent-cache-warming -o jsonpath='{.items[0].metadata.name}')

    if [ -n "$CACHE_WARMING_POD" ]; then
        ERROR_COUNT=$(kubectl logs "$CACHE_WARMING_POD" -n jorge-platform --tail=50 | grep -c "ERROR" || echo "0")
        if [ "$ERROR_COUNT" -eq 0 ]; then
            log_success "Service logs: No errors found"
        else
            log_warning "Service logs: $ERROR_COUNT errors found"
            kubectl logs "$CACHE_WARMING_POD" -n jorge-platform --tail=10 | grep "ERROR" || true
        fi
    fi

    log_success "Validation tests completed"
}

# Setup pattern analysis cron job
setup_pattern_analysis() {
    log_info "Setting up pattern analysis cron job..."

    # Apply the pattern analysis cron job
    kubectl apply -f - << EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cache-pattern-analysis-enhanced
  namespace: jorge-platform
spec:
  schedule: "0 */3 * * *"  # Every 3 hours
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: cache-pattern-analysis
        spec:
          serviceAccountName: jorge-platform-cache-service-account
          containers:
          - name: pattern-analyzer
            image: ${IMAGE_REGISTRY}/${IMAGE_NAME}:${IMAGE_TAG}
            command: ["python3", "/app/pattern_analysis_job.py"]
            args:
              - "--window-hours=72"  # 3 days of data
              - "--min-data-points=50"
              - "--verbose"
            env:
            - name: REDIS_CLUSTER_HOST
              value: "jorge-redis-cluster.cache.amazonaws.com"
            - name: REDIS_CLUSTER_PORT
              value: "6379"
            - name: REDIS_CLUSTER_MODE
              value: "true"
            - name: ENABLE_PUSHGATEWAY
              value: "true"
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway:9091"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cache-warming-secrets
                  key: redis_password
            resources:
              requests:
                cpu: "500m"
                memory: "1Gi"
              limits:
                cpu: "1"
                memory: "2Gi"
          restartPolicy: OnFailure
      backoffLimit: 3
EOF

    log_success "Pattern analysis cron job setup completed"
}

# Main deployment function
main() {
    log_info "Starting intelligent cache warming service deployment..."

    check_prerequisites
    build_and_push_image
    configure_kubectl
    deploy_kubernetes
    setup_redis_integration
    setup_monitoring
    setup_pattern_analysis
    run_validation_tests

    log_success "ðŸŽ‰ Intelligent Cache Warming Service deployment completed successfully!"
    log_info ""
    log_info "ðŸ“Š Monitoring:"
    log_info "  Service metrics: kubectl port-forward -n jorge-platform svc/intelligent-cache-warming-service 8080:80"
    log_info "  Grafana dashboards: kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80"
    log_info ""
    log_info "ðŸ”§ Management:"
    log_info "  Service logs: kubectl logs -f deployment/intelligent-cache-warming -n jorge-platform"
    log_info "  Pattern analysis logs: kubectl logs -f job/cache-pattern-analysis -n jorge-platform"
    log_info ""
    log_info "ðŸ“ˆ Expected Benefits:"
    log_info "  - >95% cache hit rates during peak traffic"
    log_info "  - 60% improvement in response times"
    log_info "  - Predictive warming 30 minutes ahead"
    log_info "  - ML-based pattern optimization"
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --env)
            DEPLOYMENT_ENV="$2"
            shift 2
            ;;
        --region)
            AWS_REGION="$2"
            shift 2
            ;;
        --cluster)
            EKS_CLUSTER_NAME="$2"
            shift 2
            ;;
        --image-tag)
            IMAGE_TAG="$2"
            shift 2
            ;;
        --registry)
            IMAGE_REGISTRY="$2"
            shift 2
            ;;
        --redis-endpoint)
            REDIS_ENDPOINT="$2"
            shift 2
            ;;
        --redis-password)
            REDIS_PASSWORD="$2"
            shift 2
            ;;
        --skip-build)
            SKIP_BUILD=true
            shift
            ;;
        --help)
            echo "Usage: $0 [OPTIONS]"
            echo "Options:"
            echo "  --env ENV               Deployment environment (default: production)"
            echo "  --region REGION         AWS region (default: us-east-1)"
            echo "  --cluster CLUSTER       EKS cluster name (default: jorge-platform-eks)"
            echo "  --image-tag TAG         Docker image tag (default: v2.0.0)"
            echo "  --registry REGISTRY     ECR registry URL"
            echo "  --redis-endpoint HOST   Redis cluster endpoint"
            echo "  --redis-password PASS   Redis cluster password"
            echo "  --skip-build           Skip Docker image build"
            echo "  --help                 Show this help"
            exit 0
            ;;
        *)
            error_exit "Unknown option: $1"
            ;;
    esac
done

# Run main function
main