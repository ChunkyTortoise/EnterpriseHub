#!/usr/bin/env python3
"""
ðŸŽ¯ GHL Finalization Agent Swarm Orchestrator
============================================

Coordinates a specialized swarm of agents to finalize the GHL Real Estate AI project.

Agent Roles:
1. **Alpha - Code Auditor**: Reviews code quality, identifies issues
2. **Beta - Test Completer**: Implements missing test logic (TODOs)
3. **Gamma - Integration Validator**: Validates all service integrations
4. **Delta - Documentation Finalizer**: Ensures all docs are complete
5. **Epsilon - Deployment Preparer**: Prepares project for production

Author: Agent Swarm System
Date: 2026-01-05
"""

import asyncio
import json
import subprocess
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set


class AgentRole(Enum):
    """Agent role definitions"""

    ALPHA = "code_auditor"
    BETA = "test_completer"
    GAMMA = "integration_validator"
    DELTA = "documentation_finalizer"
    EPSILON = "deployment_preparer"
    PHI = "frontend_engineer"


class TaskStatus(Enum):
    """Task status tracking"""

    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    BLOCKED = "blocked"


@dataclass
class Task:
    """Individual task definition"""

    id: str
    title: str
    description: str
    assigned_to: AgentRole
    status: TaskStatus = TaskStatus.PENDING
    dependencies: List[str] = field(default_factory=list)
    priority: int = 1  # 1=highest, 5=lowest
    estimated_time: int = 10  # minutes
    result: Optional[Dict] = None
    error: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


@dataclass
class Agent:
    """Agent definition"""

    role: AgentRole
    name: str
    description: str
    capabilities: List[str]
    tasks: List[Task] = field(default_factory=list)
    status: str = "idle"  # idle, working, completed, error


import ghl_real_estate_ai.agent_system.skills.codebase
import ghl_real_estate_ai.agent_system.skills.frontend
import ghl_real_estate_ai.agent_system.skills.ghl_bridge
from ghl_real_estate_ai.agent_system.hooks.governance import governance_auditor
from ghl_real_estate_ai.agent_system.hooks.real_estate import SentimentDecoder
from ghl_real_estate_ai.agent_system.hooks.security import SecuritySentry
from ghl_real_estate_ai.agent_system.skills.base import registry as skill_registry
from ghl_real_estate_ai.agents.blackboard import SharedBlackboard
from ghl_real_estate_ai.agents.traceability import trace_agent_action
from ghl_real_estate_ai.core.llm_client import LLMClient
from ghl_real_estate_ai.services.ghl_client import GHLClient
from ghl_real_estate_ai.services.roi_engine import roi_engine


class RecoveryOrchestrator:
    """
    Handles GHL API failures (429s/500s) by proposing alternative paths.
    Logs recovery events to AUDIT_MANIFEST.md.
    """

    def __init__(self, llm: LLMClient):
        self.llm = llm

    async def propose_recovery(self, error_context: Dict[str, Any], blackboard: SharedBlackboard) -> str:
        """
        Proposes an alternative path when a GHL action fails.
        """
        timestamp = datetime.now().isoformat()
        agent_name = error_context.get("agent", "Unknown")
        error_msg = error_context.get("error", "Unknown error")
        failed_action = error_context.get("action", "Unknown action")

        print(f"ðŸ©¹ RecoveryOrchestrator triggered for {failed_action} (Error: {error_msg})")

        prompt = f"""
        A GHL API action has failed in the agent swarm.
        
        Failed Action: {failed_action}
        Error: {error_msg}
        Agent: {agent_name}
        
        Context from Blackboard:
        {blackboard.get_full_context()[-1000:]}
        
        Propose a recovery strategy. If it was a communication failure (e.g. SMS failed), 
        suggest an alternative channel (Email, Task Creation, or Manual Follow-up).
        
        Provide a concise recovery plan (max 2 sentences).
        """

        response = await self.llm.agenerate(prompt=prompt, model="gemini-2.0-flash", temperature=0.1)
        recovery_plan = response.content.strip()

        # Log to Audit Manifest
        self.log_recovery_event(failed_action, error_msg, recovery_plan)

        # Write to blackboard
        blackboard.write(f"recovery_plan_{failed_action}", recovery_plan, "RecoveryOrchestrator")

        return recovery_plan

    def log_recovery_event(self, action: str, error: str, resolution: str):
        """Logs a recovery event to the manifest."""
        try:
            governance_auditor.log_recovery_event(action, error, resolution)
        except:
            pass


class ConflictResolver:
    """
    Identifies and resolves contradictions on the Shared Blackboard.
    Used when multiple agents propose conflicting actions or state updates.
    """

    def __init__(self, llm: LLMClient):
        self.llm = llm

    async def resolve(self, blackboard: SharedBlackboard) -> List[str]:
        """
        Scans blackboard history for potential conflicts and resolves them.
        Returns a list of resolution summaries.
        """
        history = blackboard.get_history()
        # Find recent duplicate writes to same keys or related keys
        recent_entries = history[-10:] if len(history) > 10 else history

        # Simple heuristic: Check for multiple agents writing to the same key in a short window
        key_map = {}
        for entry in recent_entries:
            key = entry["key"]
            if key not in key_map:
                key_map[key] = []
            key_map[key].append(entry)

        resolutions = []
        for key, entries in key_map.items():
            if len(set(e["agent"] for e in entries)) > 1:
                # Potential conflict: Multiple agents writing to the same key
                print(f"âš–ï¸ Conflict detected on key '{key}' between {set(e['agent'] for e in entries)}")

                resolution = await self._resolve_with_llm(key, entries)
                blackboard.write(f"resolution_{key}", resolution, "ConflictResolver")

                agents = list(set(e["agent"] for e in entries))
                governance_auditor.log_conflict_resolution(agents, f"Multiple agents wrote to {key}", resolution)
                resolutions.append(f"Resolved {key}: {resolution}")

        return resolutions

    async def _resolve_with_llm(self, key: str, entries: List[Dict]) -> str:
        conflict_desc = "\n".join([f"- Agent {e['agent']}: {e['value']}" for e in entries])

        prompt = f"""
        A conflict has been detected on the Shared Blackboard for the key '{key}'.
        Multiple agents have provided different values:
        
        {conflict_desc}
        
        As the ConflictResolver, your job is to synthesize these inputs into a single, 
        consistent truth or decide which agent's input takes precedence based on the context.
        
        Provide the final resolved value for the key '{key}'.
        Reply ONLY with the resolved value.
        """

        response = await self.llm.agenerate(prompt=prompt, model="gemini-2.0-flash", temperature=0.1)
        return response.content.strip()


class SwarmOrchestrator:
    """
    Orchestrates the agent swarm for GHL project finalization
    """

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.agents: Dict[AgentRole, Agent] = {}
        self.tasks: Dict[str, Task] = {}
        self.task_graph: Dict[str, List[str]] = {}
        self.completed_tasks: Set[str] = set()
        self.blackboard = SharedBlackboard()
        self.llm = LLMClient()

        # Hardening Hooks (Phase 4)
        self.security_sentry = SecuritySentry()
        self.sentiment_decoder = SentimentDecoder()
        self.conflict_resolver = ConflictResolver(self.llm)
        self.recovery_orchestrator = RecoveryOrchestrator(self.llm)

        self._initialize_agents()
        self._initialize_tasks()

    @trace_agent_action(action_name="task_reflection")
    async def reflect_on_result(self, task: Task, context: str, result_content: str) -> bool:
        """
        Self-correction mechanism: The agent reviews its own work.
        Returns True if satisfied, False if rework is needed.
        """
        reflection_prompt = f"""
        You are a Quality Assurance Auditor. Review the following task execution.
        
        Task: {task.title}
        Objective: {task.description}
        
        Execution Result:
        {result_content}
        
        Context:
        {context}
        
        Did the agent successfully achieve the objective? 
        If yes, reply with 'APPROVED'.
        If no, reply with 'REJECTED: <reason>'.
        """

        response = await self.llm.agenerate(
            prompt=reflection_prompt,
            model="gemini-2.0-flash",  # Always use high-reasoning model for reflection
            temperature=0.1,
        )

        if "APPROVED" in response.content:
            print(f"âœ… Reflection PASSED for task {task.id}")
            return True
        else:
            print(f"âš ï¸ Reflection FAILED for task {task.id}: {response.content}")
            return False

    @trace_agent_action(action_name="task_execution")
    async def execute_task(self, task_id: str, complexity: str = "medium", risk: str = "low"):
        """
        Execute a task using a Recursive Tool Loop (ReAct).
        Supports multi-turn tool use and HITL approval for high-risk tasks.
        """
        task = self.tasks[task_id]
        agent = self.agents[task.assigned_to]

        # HITL Gatekeeping
        if risk == "high":
            confirm = input(f"âš ï¸ [HITL] Task {task_id} is HIGH RISK. Approve execution? (y/n): ")
            if confirm.lower() != "y":
                task.status = TaskStatus.BLOCKED
                self.blackboard.write(f"task_blocked_{task_id}", "User denied high-risk execution", agent.name)
                return

        print(f"ðŸš€ Executing task {task_id}: {task.title} (Agent: {agent.name})")
        task.status = TaskStatus.IN_PROGRESS
        task.started_at = datetime.now()

        # Dynamic Model Selection
        model_name = "gemini-2.0-flash" if complexity == "high" else self.llm.model

        context = self.blackboard.get_full_context()
        skills = skill_registry.find_relevant_skills(task.description)
        skill_tools = [s.to_gemini_tool()["function_declarations"][0] for s in skills]

        # Maintain conversation history
        messages = [
            {
                "role": "user",
                "content": f"Objective: {task.description}\n\nCurrent Blackboard Context:\n{context}\n\nPlease achieve the objective. Use available tools if needed.",
            }
        ]

        # RECURSIVE TOOL LOOP (Max 10 turns)
        final_response = ""
        for turn in range(10):
            response = await self.llm.agenerate(
                prompt=messages[-1]["content"],
                history=messages[:-1],
                system_prompt=f"You are {agent.name}, an expert {agent.description}. Use tools provided to achieve the objective.",
                tools=skill_tools if skill_tools else None,
                model=model_name,
            )

            # Record response in messages for context maintenance
            if response.content:
                # ðŸ›¡ï¸ Security Check (Phase 4 Hardening)
                if not self.security_sentry.scan_output(response.content, agent_name=agent.name):
                    print(f"ðŸš¨ SECURITY VIOLATION detected in {agent.name} output! Blocking task.")
                    self.blackboard.write(f"security_violation_{task_id}", "PII or Injection detected", agent.name)
                    task.status = TaskStatus.FAILED
                    task.error = "Security violation detected"
                    return {"error": "Security violation"}

                messages.append({"role": "assistant", "content": response.content})
                final_response = response.content

            # Handle Tool Calls
            if response.tool_calls:
                tool_results = []
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]

                    print(f"ðŸ› ï¸ Agent {agent.name} calling tool: {tool_name}({tool_args})")
                    self.blackboard.write(
                        f"tool_call_{task_id}_t{turn}", {"tool": tool_name, "args": tool_args}, agent.name
                    )

                    skill = skill_registry.get_skill(tool_name)
                    if skill:
                        try:
                            result = skill.execute(**tool_args)
                            print(f"âœ… Tool {tool_name} returned: {str(result)[:100]}...")
                            tool_results.append({"tool": tool_name, "result": result})
                        except Exception as e:
                            print(f"âŒ Tool {tool_name} failed: {e}")
                            recovery_plan = await self.recovery_orchestrator.propose_recovery(
                                {"agent": agent.name, "action": tool_name, "error": str(e)}, self.blackboard
                            )
                            tool_results.append({"tool": tool_name, "error": str(e), "recovery_plan": recovery_plan})
                    else:
                        tool_results.append({"tool": tool_name, "error": "Skill not found"})

                # Add tool results back to context
                messages.append({"role": "user", "content": f"Tool Results: {json.dumps(tool_results)}"})

                # Continue loop to allow agent to process results
                continue

            # If no tool calls and we have content, we are done
            if response.content:
                break

        # Automated Reflection (Phase 4 Enhancement)
        # Verify the result before marking complete
        if complexity == "high" or risk == "high" or True:  # Enable for all for now to test
            approved = await self.reflect_on_result(task, context, final_response)
            if not approved:
                # In a full implementation, this would trigger a retry loop or human escalation
                # For now, we log the failure but mark as completed (with warning) to avoid infinite loops
                self.blackboard.write(
                    f"task_reflection_failed_{task_id}", "Auto-reflection rejected the result", agent.name
                )
                task.error = "Auto-reflection rejected the result"

        # Post-execution Reflection & Blackboard Update
        task.status = TaskStatus.COMPLETED
        task.completed_at = datetime.now()
        task.result = {"content": final_response, "messages_count": len(messages)}

        # ðŸ§  Sentiment Analysis (Phase 4 Hardening)
        sentiment = self.sentiment_decoder.analyze(final_response)
        task.result["sentiment"] = sentiment["sentiment"]
        if sentiment["emotional_state"] == "volatile":
            print(f"âš ï¸ VOLATILE sentiment detected in result for {task_id}")
            self.blackboard.write(f"task_warning_{task_id}", "Volatile sentiment detected in output", agent.name)

        self.completed_tasks.add(task_id)

        self.blackboard.write(f"task_result_{task_id}", task.result, agent.name)

        return task.result

    def _initialize_agents(self):
        """Initialize all agents with their capabilities"""

        # Alpha - Code Auditor
        self.agents[AgentRole.ALPHA] = Agent(
            role=AgentRole.ALPHA,
            name="Alpha Code Auditor",
            description="Reviews code quality, identifies issues, ensures best practices",
            capabilities=[
                "Code quality analysis",
                "Security vulnerability scanning",
                "Performance optimization identification",
                "Code smell detection",
                "Dependency analysis",
            ],
        )

        # Beta - Test Completer
        self.agents[AgentRole.BETA] = Agent(
            role=AgentRole.BETA,
            name="Beta Test Completer",
            description="Implements missing test logic and ensures test coverage",
            capabilities=[
                "TODO resolution in tests",
                "Test logic implementation",
                "Test coverage analysis",
                "Mock creation",
                "Assertion writing",
            ],
        )

        # Gamma - Integration Validator
        self.agents[AgentRole.GAMMA] = Agent(
            role=AgentRole.GAMMA,
            name="Gamma Integration Validator",
            description="Validates all service integrations and API connections",
            capabilities=[
                "Service integration testing",
                "API endpoint validation",
                "Database connection verification",
                "External service connectivity",
                "Error handling validation",
            ],
        )

        # Delta - Documentation Finalizer
        self.agents[AgentRole.DELTA] = Agent(
            role=AgentRole.DELTA,
            name="Delta Documentation Finalizer",
            description="Ensures all documentation is complete and accurate",
            capabilities=[
                "README updates",
                "API documentation",
                "Inline documentation",
                "Architecture diagrams",
                "User guides",
            ],
        )

        # Epsilon - Deployment Preparer
        self.agents[AgentRole.EPSILON] = Agent(
            role=AgentRole.EPSILON,
            name="Epsilon Deployment Preparer",
            description="Prepares project for production deployment",
            capabilities=[
                "Environment configuration",
                "Dependency management",
                "Build process validation",
                "Deployment scripts",
                "Production readiness checklist",
            ],
        )

        # Phi - Frontend Engineer
        self.agents[AgentRole.PHI] = Agent(
            role=AgentRole.PHI,
            name="Phi Frontend Engineer",
            description="Generates production-grade UI, dashboards, and visualizations",
            capabilities=[
                "Shadcn/UI component generation",
                "Tremor dashboard construction",
                "Next.js page routing",
                "Data visualization (Recharts/ECharts)",
                "Frontend state management",
                "Hot-reload preview management",
                "Visual regression testing",
                "Semantic React Refactoring",
            ],
        )

    def _initialize_tasks(self):
        """Initialize all finalization tasks with dependencies"""

        tasks = [
            # Phase 1: Analysis & Planning
            Task(
                id="task_001",
                title="Project Structure Analysis",
                description="Analyze complete project structure and identify all components",
                assigned_to=AgentRole.ALPHA,
                priority=1,
                estimated_time=5,
            ),
            Task(
                id="task_002",
                title="Test TODO Identification",
                description="Identify all TODO comments in test files",
                assigned_to=AgentRole.BETA,
                priority=1,
                estimated_time=5,
            ),
            # Phase 2: Code Quality
            Task(
                id="task_003",
                title="Code Quality Audit",
                description="Run comprehensive code quality checks",
                assigned_to=AgentRole.ALPHA,
                dependencies=["task_001"],
                priority=1,
                estimated_time=15,
            ),
            Task(
                id="task_004",
                title="Security Vulnerability Scan",
                description="Scan for security vulnerabilities",
                assigned_to=AgentRole.ALPHA,
                dependencies=["task_001"],
                priority=1,
                estimated_time=10,
            ),
            # Phase 3: Test Completion
            Task(
                id="task_005",
                title="Implement Test Logic - Reengagement",
                description="Complete TODO items in test_reengagement_engine_extended.py",
                assigned_to=AgentRole.BETA,
                dependencies=["task_002"],
                priority=2,
                estimated_time=20,
            ),
            Task(
                id="task_006",
                title="Implement Test Logic - Memory Service",
                description="Complete TODO items in test_memory_service_extended.py",
                assigned_to=AgentRole.BETA,
                dependencies=["task_002"],
                priority=2,
                estimated_time=20,
            ),
            Task(
                id="task_007",
                title="Implement Test Logic - GHL Client",
                description="Complete TODO items in test_ghl_client_extended.py",
                assigned_to=AgentRole.BETA,
                dependencies=["task_002"],
                priority=2,
                estimated_time=20,
            ),
            Task(
                id="task_008",
                title="Implement Security Tests",
                description="Complete security-related TODO items",
                assigned_to=AgentRole.BETA,
                dependencies=["task_002"],
                priority=1,
                estimated_time=30,
            ),
            # Phase 4: Integration Validation
            Task(
                id="task_009",
                title="Validate GHL API Integration",
                description="Test all GHL API endpoints and connections",
                assigned_to=AgentRole.GAMMA,
                dependencies=["task_005", "task_006", "task_007"],
                priority=2,
                estimated_time=25,
            ),
            Task(
                id="task_010",
                title="Validate Database Connections",
                description="Test all database operations and connections",
                assigned_to=AgentRole.GAMMA,
                dependencies=["task_003"],
                priority=2,
                estimated_time=15,
            ),
            Task(
                id="task_011",
                title="Validate Service Dependencies",
                description="Verify all service-to-service integrations",
                assigned_to=AgentRole.GAMMA,
                dependencies=["task_003"],
                priority=2,
                estimated_time=20,
            ),
            # Phase 5: Documentation
            Task(
                id="task_012",
                title="Update Main README",
                description="Ensure README is complete with all features",
                assigned_to=AgentRole.DELTA,
                dependencies=["task_009", "task_010"],
                priority=3,
                estimated_time=15,
            ),
            Task(
                id="task_013",
                title="Generate API Documentation",
                description="Create/update API documentation",
                assigned_to=AgentRole.DELTA,
                dependencies=["task_009"],
                priority=3,
                estimated_time=20,
            ),
            Task(
                id="task_014",
                title="Update Service Documentation",
                description="Document all 62+ services",
                assigned_to=AgentRole.DELTA,
                dependencies=["task_011"],
                priority=3,
                estimated_time=30,
            ),
            # Phase 6: Deployment Preparation
            Task(
                id="task_015",
                title="Environment Configuration",
                description="Setup and validate environment variables",
                assigned_to=AgentRole.EPSILON,
                dependencies=["task_003", "task_004"],
                priority=1,
                estimated_time=15,
            ),
            Task(
                id="task_016",
                title="Dependency Management",
                description="Validate and freeze all dependencies",
                assigned_to=AgentRole.EPSILON,
                dependencies=["task_003"],
                priority=1,
                estimated_time=10,
            ),
            Task(
                id="task_017",
                title="Production Readiness Checklist",
                description="Complete production deployment checklist",
                assigned_to=AgentRole.EPSILON,
                dependencies=["task_012", "task_015", "task_016"],
                priority=1,
                estimated_time=20,
            ),
            Task(
                id="task_018",
                title="Deployment Scripts",
                description="Create/validate deployment scripts",
                assigned_to=AgentRole.EPSILON,
                dependencies=["task_015", "task_016"],
                priority=2,
                estimated_time=25,
            ),
            # Phase 6.5: Frontend Generation (PHI)
            Task(
                id="task_021",
                title="Generate Core UI Components",
                description="Generate Shadcn-based UI components for Enterprise Hub",
                assigned_to=AgentRole.PHI,
                dependencies=["task_001"],
                priority=2,
                estimated_time=30,
            ),
            Task(
                id="task_022",
                title="Construct KPI Dashboard",
                description="Build interactive Tremor dashboard for executive metrics",
                assigned_to=AgentRole.PHI,
                dependencies=["task_021", "task_013"],
                priority=1,
                estimated_time=45,
            ),
            Task(
                id="task_023",
                title="Integrate AI State Sync",
                description="Implement global state sync for AI insights in the frontend",
                assigned_to=AgentRole.PHI,
                dependencies=["task_022"],
                priority=2,
                estimated_time=40,
            ),
            Task(
                id="task_024",
                title="Visual QA & Self-Correction Loop",
                description="Execute an autonomous visual QA loop using Playwright and Vision analysis to verify and correct the generated dashboard components.",
                assigned_to=AgentRole.PHI,
                dependencies=["task_023"],
                priority=2,
                estimated_time=15,
            ),
            # Phase 7: Final Validation
            Task(
                id="task_019",
                title="Run Full Test Suite",
                description="Execute all tests and verify passing",
                assigned_to=AgentRole.GAMMA,
                dependencies=["task_005", "task_006", "task_007", "task_008"],
                priority=1,
                estimated_time=30,
            ),
            Task(
                id="task_020",
                title="Final Integration Test",
                description="Run end-to-end integration tests",
                assigned_to=AgentRole.GAMMA,
                dependencies=["task_019"],
                priority=1,
                estimated_time=25,
            ),
        ]

        # Store tasks
        for task in tasks:
            self.tasks[task.id] = task
            self.agents[task.assigned_to].tasks.append(task)

        # Build task graph
        self._build_task_graph()

    def _build_task_graph(self):
        """Build task dependency graph"""
        for task_id, task in self.tasks.items():
            self.task_graph[task_id] = task.dependencies

    def get_ready_tasks(self) -> List[Task]:
        """Get all tasks ready to execute (no pending dependencies)"""
        ready = []
        for task_id, task in self.tasks.items():
            if task.status == TaskStatus.PENDING:
                # Check if all dependencies are completed
                deps_completed = all(dep_id in self.completed_tasks for dep_id in task.dependencies)
                if deps_completed:
                    ready.append(task)

        # Sort by priority (lower number = higher priority)
        ready.sort(key=lambda t: (t.priority, t.id))
        return ready

    def get_status_report(self) -> Dict:
        """Generate comprehensive status report"""
        total_tasks = len(self.tasks)
        completed = len([t for t in self.tasks.values() if t.status == TaskStatus.COMPLETED])
        in_progress = len([t for t in self.tasks.values() if t.status == TaskStatus.IN_PROGRESS])
        failed = len([t for t in self.tasks.values() if t.status == TaskStatus.FAILED])
        blocked = len([t for t in self.tasks.values() if t.status == TaskStatus.BLOCKED])

        agent_stats = {}
        for role, agent in self.agents.items():
            agent_tasks = agent.tasks
            agent_stats[role.value] = {
                "total": len(agent_tasks),
                "completed": len([t for t in agent_tasks if t.status == TaskStatus.COMPLETED]),
                "in_progress": len([t for t in agent_tasks if t.status == TaskStatus.IN_PROGRESS]),
                "pending": len([t for t in agent_tasks if t.status == TaskStatus.PENDING]),
            }

        return {
            "timestamp": datetime.now().isoformat(),
            "overall": {
                "total_tasks": total_tasks,
                "completed": completed,
                "in_progress": in_progress,
                "failed": failed,
                "blocked": blocked,
                "progress_percentage": (completed / total_tasks * 100) if total_tasks > 0 else 0,
            },
            "agents": agent_stats,
            "ready_tasks": len(self.get_ready_tasks()),
        }

    def print_status(self):
        """Print formatted status report"""
        status = self.get_status_report()

        print("\n" + "=" * 80)
        print("ðŸŽ¯ GHL PROJECT FINALIZATION - AGENT SWARM STATUS")
        print("=" * 80)
        print(f"\nðŸ“Š Overall Progress: {status['overall']['progress_percentage']:.1f}%")
        print(f"   âœ… Completed: {status['overall']['completed']}/{status['overall']['total_tasks']}")
        print(f"   ðŸ”„ In Progress: {status['overall']['in_progress']}")
        print(
            f"   â³ Pending: {status['overall']['total_tasks'] - status['overall']['completed'] - status['overall']['in_progress']}"
        )
        print(f"   âŒ Failed: {status['overall']['failed']}")
        print(f"   ðŸš« Blocked: {status['overall']['blocked']}")

        print("\nðŸ‘¥ Agent Status:")
        for agent_role, agent in self.agents.items():
            stats = status["agents"][agent_role.value]
            print(f"\n   {agent.name}:")
            print(f"      Status: {agent.status}")
            print(f"      Tasks: {stats['completed']}/{stats['total']} completed")
            if stats["in_progress"] > 0:
                print(f"      Currently working on {stats['in_progress']} task(s)")

        ready_tasks = self.get_ready_tasks()
        if ready_tasks:
            print(f"\nðŸš€ Next {len(ready_tasks)} task(s) ready to execute:")
            for task in ready_tasks[:5]:  # Show first 5
                print(f"   â€¢ [{task.id}] {task.title} -> {task.assigned_to.value}")

        print("\n" + "=" * 80 + "\n")

    def generate_execution_plan(self) -> List[Dict]:
        """Generate execution plan showing task order"""
        # ... (keep existing implementation)
        return []  # Placeholder as we are adding a new method below

    def _calculate_complexity(self) -> float:
        """
        Calculates a complexity score (0.0 to 1.0) based on blackboard state.
        Factors: Context length, number of completed tasks, tool usage density.
        """
        context = self.blackboard.get_full_context()
        context_score = min(len(context) / 10000, 1.0)

        task_score = len(self.completed_tasks) / len(self.tasks) if self.tasks else 0

        # Simple weighted average
        complexity = (context_score * 0.7) + (task_score * 0.3)
        return complexity

    async def run_parallel_swarm(self):
        """
        Executes the swarm tasks in parallel where dependencies allow.
        Dynamically adjusts concurrency based on blackboard complexity.
        """
        print("\nðŸš€ Starting Parallel Swarm Execution (Adaptive Scaling)...")

        while len(self.completed_tasks) < len(self.tasks):
            ready_tasks = self.get_ready_tasks()
            if not ready_tasks:
                if any(t.status == TaskStatus.FAILED for t in self.tasks.values()):
                    print("âŒ Swarm halted due to task failures.")
                    break
                print("ðŸ No more tasks ready. Swarm complete.")
                break

            complexity = self._calculate_complexity()
            # Adaptive Scaling: High complexity = lower concurrency to ensure stability/reasoning quality
            # Low complexity = higher concurrency for speed.
            # Limiting to 1 for free tier stability
            max_parallel = 1
            tasks_to_run = ready_tasks[:max_parallel]

            print(
                f"ðŸ“¡ Dispatching {len(tasks_to_run)} parallel tasks (Complexity: {complexity:.2f}, Max Parallel: {max_parallel})..."
            )

            # Execute selected tasks in parallel
            await asyncio.gather(*(self.execute_task(t.id) for t in tasks_to_run))

            # Rate limit buffer - Free tier is very restrictive
            await asyncio.sleep(10)

            # Autonomous Conflict Resolution (Phase 6)
            resolutions = await self.conflict_resolver.resolve(self.blackboard)
            if resolutions:
                print(f"âš–ï¸ Autonomous Conflict Resolution applied: {len(resolutions)} keys resolved.")

        print("\nâœ¨ Parallel Swarm Execution Finished.")

        # Calculate ROI (Phase 7)
        swarm_stats = {
            "tasks_completed": len(self.completed_tasks),
            "matches_found": self.blackboard.read("property_matches_count") or 0,  # Example key
        }
        roi_results = roi_engine.calculate_swarm_roi(swarm_stats, agent_name="SwarmOrchestrator")
        print(f"ðŸ’° ROI Generated: ${roi_results['total_value_generated']}")

        self.print_status()


def main():
    """Main execution"""
    project_root = Path(__file__).parent.parent
    orchestrator = SwarmOrchestrator(project_root)

    print("\nðŸš€ GHL Project Finalization - Agent Swarm Initialized")
    print("=" * 80)

    # Print initial status
    orchestrator.print_status()

    # Generate execution plan
    plan = orchestrator.generate_execution_plan()

    print("ðŸ“‹ EXECUTION PLAN")
    print("=" * 80)
    total_time = sum(task["estimated_time"] for task in plan)
    print(f"\nTotal estimated time: {total_time} minutes ({total_time / 60:.1f} hours)")
    print(f"Total tasks: {len(plan)}\n")

    # Group by phase
    phases = {
        1: "Analysis & Planning",
        2: "Code Quality",
        3: "Test Completion",
        4: "Integration Validation",
        5: "Documentation",
        6: "Deployment Preparation",
        7: "Final Validation",
    }

    current_phase = 1
    for i, task in enumerate(plan, 1):
        # Detect phase changes (rough heuristic)
        if i > 2 and current_phase == 1:
            current_phase = 2
        elif i > 4 and current_phase == 2:
            current_phase = 3
        elif i > 8 and current_phase == 3:
            current_phase = 4
        elif i > 11 and current_phase == 4:
            current_phase = 5
        elif i > 14 and current_phase == 5:
            current_phase = 6
        elif i > 18 and current_phase == 6:
            current_phase = 7

        if i == 1 or (i > 2 and plan[i - 2].get("phase") != current_phase):
            phase_name = phases.get(current_phase, "Unknown")
            print(f"--- PHASE {current_phase}: {phase_name} ---")

        print(f"{i:2}. [{task['id']}] {task['title']} ({task['assigned_to']})")


if __name__ == "__main__":
    main()
