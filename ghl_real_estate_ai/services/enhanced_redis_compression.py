#!/usr/bin/env python3
"""
üóúÔ∏è Enhanced Redis Compression Service for EnterpriseHub
======================================================

Advanced compression strategies for 40% memory reduction and cost optimization.

Performance Targets:
- 40% memory reduction through intelligent compression
- -30% Redis hosting costs
- <10ms compression/decompression overhead
- 60-80% size reduction for large objects

Features:
- Multi-algorithm compression (LZ4, ZSTD, Brotli)
- Adaptive compression based on object size and type
- Compression ratio analytics and optimization
- Memory usage monitoring and alerts
- Automatic compression tuning
- Cost savings tracking

Author: EnterpriseHub Performance Agent
Date: 2026-01-10
"""

import asyncio
import time
import json
import pickle
import hashlib
import zstandard as zstd
import brotli
import lz4.frame
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class CompressionAlgorithm(Enum):
    """Available compression algorithms with characteristics."""
    NONE = "none"
    LZ4 = "lz4"          # Fast, moderate compression
    ZSTD = "zstd"        # Balanced speed/ratio
    BROTLI = "brotli"    # Best compression, slower


@dataclass
class CompressionProfile:
    """Compression profile for different object types."""
    name: str
    algorithm: CompressionAlgorithm
    threshold_kb: float
    compression_level: int
    max_size_mb: float  # Don't compress objects larger than this
    target_ratio: float  # Target compression ratio


@dataclass
class CompressionMetrics:
    """Compression performance and savings metrics."""
    total_objects: int = 0
    compressed_objects: int = 0
    bytes_before_compression: int = 0
    bytes_after_compression: int = 0
    total_compression_time_ms: float = 0.0
    total_decompression_time_ms: float = 0.0
    algorithm_usage: Dict[str, int] = field(default_factory=dict)
    cost_savings_usd_per_month: float = 0.0

    @property
    def compression_ratio(self) -> float:
        """Overall compression ratio."""
        if self.bytes_before_compression == 0:
            return 1.0
        return self.bytes_before_compression / self.bytes_after_compression

    @property
    def memory_savings_percent(self) -> float:
        """Memory savings percentage."""
        if self.bytes_before_compression == 0:
            return 0.0
        savings = self.bytes_before_compression - self.bytes_after_compression
        return (savings / self.bytes_before_compression) * 100

    @property
    def avg_compression_time_ms(self) -> float:
        """Average compression time per object."""
        if self.compressed_objects == 0:
            return 0.0
        return self.total_compression_time_ms / self.compressed_objects


class EnhancedRedisCompression:
    """
    Advanced Redis compression with multi-algorithm support and optimization.

    Compression Strategy:
    - Small objects (<1KB): No compression (overhead > benefit)
    - Medium objects (1-10KB): LZ4 (fast, good for hot data)
    - Large objects (10-100KB): ZSTD (balanced speed/ratio)
    - Very large objects (>100KB): Brotli (best compression)

    Performance Optimizations:
    - Algorithm selection based on object size and access pattern
    - Adaptive compression levels based on performance feedback
    - Compression ratio tracking and tuning
    - Memory usage monitoring and cost calculation
    """

    def __init__(self):
        # Compression profiles for different use cases
        self.profiles = {
            'hot_data': CompressionProfile(
                name='hot_data',
                algorithm=CompressionAlgorithm.LZ4,
                threshold_kb=1.0,
                compression_level=1,  # Fastest
                max_size_mb=50.0,
                target_ratio=1.7
            ),
            'warm_data': CompressionProfile(
                name='warm_data',
                algorithm=CompressionAlgorithm.ZSTD,
                threshold_kb=2.0,
                compression_level=3,  # Balanced
                max_size_mb=100.0,
                target_ratio=2.5
            ),
            'cold_data': CompressionProfile(
                name='cold_data',
                algorithm=CompressionAlgorithm.BROTLI,
                threshold_kb=5.0,
                compression_level=6,  # Best compression
                max_size_mb=500.0,
                target_ratio=3.5
            ),
            'ml_models': CompressionProfile(
                name='ml_models',
                algorithm=CompressionAlgorithm.ZSTD,
                threshold_kb=10.0,
                compression_level=6,  # High compression for models
                max_size_mb=1000.0,
                target_ratio=4.0
            )
        }

        # Performance tracking
        self.metrics = CompressionMetrics()
        self.object_type_patterns = {
            'conversation_history': r'conv:.*',
            'lead_profiles': r'lead:.*',
            'property_data': r'prop:.*',
            'ml_features': r'ml:features:.*',
            'ml_models': r'ml:model:.*',
            'coaching_data': r'coaching:.*',
            'analytics': r'analytics:.*'
        }

        # Compression cache for frequently compressed data
        self.compression_cache: Dict[str, bytes] = {}
        self.cache_hit_stats = {'hits': 0, 'misses': 0}

        # Cost tracking (Redis pricing: ~$0.086 per GB-hour)
        self.redis_cost_per_gb_hour = 0.086  # AWS ElastiCache pricing

        logger.info("Enhanced Redis compression initialized with multi-algorithm support")

    async def compress_data(
        self,
        data: Any,
        key: str,
        access_pattern: str = 'warm_data',
        force_algorithm: Optional[CompressionAlgorithm] = None
    ) -> Tuple[bytes, Dict[str, Any]]:
        """
        Compress data using optimal algorithm based on size and access pattern.

        Args:
            data: Data to compress
            key: Redis key (used for pattern detection)
            access_pattern: Expected access pattern (hot_data, warm_data, cold_data)
            force_algorithm: Force specific algorithm

        Returns:
            Tuple of (compressed_data, metadata)
        """

        start_time = time.perf_counter()

        # Serialize data efficiently
        if isinstance(data, bytes):
            serialized_data = data
        elif isinstance(data, str):
            serialized_data = data.encode('utf-8')
        else:
            # Use highest protocol for efficiency
            serialized_data = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)

        original_size = len(serialized_data)
        original_size_kb = original_size / 1024.0

        # Detect object type from key pattern
        object_type = self._detect_object_type(key)

        # Select compression profile
        profile = self._select_compression_profile(
            object_type, original_size_kb, access_pattern, force_algorithm
        )

        # Check if compression is beneficial
        if original_size_kb < profile.threshold_kb:
            # Too small to compress
            compression_time_ms = (time.perf_counter() - start_time) * 1000
            metadata = {
                'compressed': False,
                'algorithm': 'none',
                'original_size': original_size,
                'compression_ratio': 1.0,
                'compression_time_ms': compression_time_ms,
                'reason': 'below_threshold'
            }
            self._update_metrics(original_size, original_size, compression_time_ms, 0, 'none')
            return serialized_data, metadata

        if original_size_kb > profile.max_size_mb * 1024:
            # Too large to compress efficiently
            compression_time_ms = (time.perf_counter() - start_time) * 1000
            metadata = {
                'compressed': False,
                'algorithm': 'none',
                'original_size': original_size,
                'compression_ratio': 1.0,
                'compression_time_ms': compression_time_ms,
                'reason': 'too_large'
            }
            self._update_metrics(original_size, original_size, compression_time_ms, 0, 'none')
            return serialized_data, metadata

        # Check compression cache for frequently accessed data
        data_hash = hashlib.sha256(serialized_data).hexdigest()[:16]
        cache_key = f"{profile.algorithm.value}:{data_hash}"

        if cache_key in self.compression_cache:
            self.cache_hit_stats['hits'] += 1
            cached_compressed = self.compression_cache[cache_key]
            compression_time_ms = (time.perf_counter() - start_time) * 1000

            metadata = {
                'compressed': True,
                'algorithm': profile.algorithm.value,
                'original_size': original_size,
                'compressed_size': len(cached_compressed),
                'compression_ratio': original_size / len(cached_compressed),
                'compression_time_ms': compression_time_ms,
                'cached': True
            }
            return cached_compressed, metadata

        # Perform compression
        self.cache_hit_stats['misses'] += 1
        compressed_data = await self._compress_with_algorithm(
            serialized_data, profile.algorithm, profile.compression_level
        )

        compression_time_ms = (time.perf_counter() - start_time) * 1000
        compressed_size = len(compressed_data)
        compression_ratio = original_size / compressed_size

        # Only use compression if it provides significant benefit (>20% reduction)
        if compression_ratio < 1.2:
            metadata = {
                'compressed': False,
                'algorithm': 'none',
                'original_size': original_size,
                'compression_ratio': 1.0,
                'compression_time_ms': compression_time_ms,
                'reason': 'insufficient_benefit'
            }
            self._update_metrics(original_size, original_size, compression_time_ms, 0, 'none')
            return serialized_data, metadata

        # Cache compression result for hot data
        if access_pattern == 'hot_data' and len(self.compression_cache) < 1000:
            self.compression_cache[cache_key] = compressed_data

        # Create metadata
        metadata = {
            'compressed': True,
            'algorithm': profile.algorithm.value,
            'compression_level': profile.compression_level,
            'original_size': original_size,
            'compressed_size': compressed_size,
            'compression_ratio': compression_ratio,
            'compression_time_ms': compression_time_ms,
            'object_type': object_type,
            'profile': profile.name
        }

        # Update metrics
        self._update_metrics(
            original_size, compressed_size, compression_time_ms, 0, profile.algorithm.value
        )

        logger.debug(
            f"Compressed {key}: {original_size} ‚Üí {compressed_size} bytes "
            f"({compression_ratio:.2f}x, {profile.algorithm.value})"
        )

        return compressed_data, metadata

    async def decompress_data(
        self,
        compressed_data: bytes,
        metadata: Dict[str, Any]
    ) -> Any:
        """
        Decompress data using metadata information.

        Args:
            compressed_data: Compressed data bytes
            metadata: Compression metadata

        Returns:
            Decompressed and deserialized data
        """

        start_time = time.perf_counter()

        if not metadata.get('compressed', False):
            # Data is not compressed
            decompression_time_ms = (time.perf_counter() - start_time) * 1000
            self.metrics.total_decompression_time_ms += decompression_time_ms

            # Deserialize based on type
            try:
                return pickle.loads(compressed_data)
            except:
                # Try as string if pickle fails
                try:
                    return compressed_data.decode('utf-8')
                except:
                    return compressed_data

        # Decompress based on algorithm
        algorithm = metadata.get('algorithm', 'lz4')

        try:
            if algorithm == 'lz4':
                decompressed_data = lz4.frame.decompress(compressed_data)
            elif algorithm == 'zstd':
                decompressor = zstd.ZstdDecompressor()
                decompressed_data = decompressor.decompress(compressed_data)
            elif algorithm == 'brotli':
                decompressed_data = brotli.decompress(compressed_data)
            else:
                raise ValueError(f"Unknown compression algorithm: {algorithm}")

            decompression_time_ms = (time.perf_counter() - start_time) * 1000
            self.metrics.total_decompression_time_ms += decompression_time_ms

            # Deserialize
            try:
                return pickle.loads(decompressed_data)
            except:
                try:
                    return decompressed_data.decode('utf-8')
                except:
                    return decompressed_data

        except Exception as e:
            logger.error(f"Decompression failed with {algorithm}: {e}")
            raise

    async def _compress_with_algorithm(
        self,
        data: bytes,
        algorithm: CompressionAlgorithm,
        level: int
    ) -> bytes:
        """Compress data using specified algorithm."""

        if algorithm == CompressionAlgorithm.LZ4:
            return lz4.frame.compress(
                data,
                compression_level=level,
                auto_flush=True
            )
        elif algorithm == CompressionAlgorithm.ZSTD:
            compressor = zstd.ZstdCompressor(level=level)
            return compressor.compress(data)
        elif algorithm == CompressionAlgorithm.BROTLI:
            return brotli.compress(data, quality=level)
        else:
            raise ValueError(f"Unsupported compression algorithm: {algorithm}")

    def _detect_object_type(self, key: str) -> str:
        """Detect object type from Redis key pattern."""
        import re

        for obj_type, pattern in self.object_type_patterns.items():
            if re.match(pattern, key):
                return obj_type

        return 'generic'

    def _select_compression_profile(
        self,
        object_type: str,
        size_kb: float,
        access_pattern: str,
        force_algorithm: Optional[CompressionAlgorithm]
    ) -> CompressionProfile:
        """Select optimal compression profile based on context."""

        if force_algorithm:
            # Create custom profile with forced algorithm
            base_profile = self.profiles[access_pattern]
            return CompressionProfile(
                name='custom',
                algorithm=force_algorithm,
                threshold_kb=base_profile.threshold_kb,
                compression_level=base_profile.compression_level,
                max_size_mb=base_profile.max_size_mb,
                target_ratio=base_profile.target_ratio
            )

        # Special handling for ML models (use high compression)
        if object_type == 'ml_models':
            return self.profiles['ml_models']

        # Size-based selection with access pattern consideration
        if size_kb < 10:  # Small objects
            return self.profiles['hot_data']
        elif size_kb < 100:  # Medium objects
            return self.profiles[access_pattern]
        else:  # Large objects
            return self.profiles['cold_data']

    def _update_metrics(
        self,
        original_size: int,
        final_size: int,
        compression_time_ms: float,
        decompression_time_ms: float,
        algorithm: str
    ):
        """Update compression metrics."""

        self.metrics.total_objects += 1
        self.metrics.bytes_before_compression += original_size
        self.metrics.bytes_after_compression += final_size
        self.metrics.total_compression_time_ms += compression_time_ms
        self.metrics.total_decompression_time_ms += decompression_time_ms

        if final_size < original_size:
            self.metrics.compressed_objects += 1

        # Algorithm usage tracking
        if algorithm not in self.metrics.algorithm_usage:
            self.metrics.algorithm_usage[algorithm] = 0
        self.metrics.algorithm_usage[algorithm] += 1

        # Cost savings calculation (based on memory reduction)
        memory_saved_gb = (original_size - final_size) / (1024**3)
        monthly_savings = memory_saved_gb * self.redis_cost_per_gb_hour * 24 * 30
        self.metrics.cost_savings_usd_per_month += monthly_savings

    async def optimize_compression_profiles(self) -> Dict[str, Any]:
        """
        Analyze compression performance and optimize profiles.

        Returns optimization recommendations and updated profiles.
        """

        if self.metrics.total_objects == 0:
            return {"status": "no_data", "recommendations": []}

        recommendations = []
        optimizations = {}

        # Analyze algorithm performance
        for algorithm, usage_count in self.metrics.algorithm_usage.items():
            if usage_count == 0:
                continue

            usage_percent = (usage_count / self.metrics.total_objects) * 100

            if algorithm == 'none' and usage_percent > 50:
                recommendations.append(
                    "High percentage of uncompressed objects - consider lowering thresholds"
                )
            elif algorithm == 'brotli' and usage_percent > 30:
                recommendations.append(
                    "High Brotli usage may impact performance - consider ZSTD for better speed"
                )

        # Analyze compression ratios
        overall_ratio = self.metrics.compression_ratio
        memory_savings = self.metrics.memory_savings_percent

        if memory_savings < 30:
            recommendations.append(
                "Memory savings below 30% - consider more aggressive compression"
            )
        elif memory_savings > 60:
            recommendations.append(
                f"Excellent memory savings of {memory_savings:.1f}% achieved"
            )

        # Analyze performance
        avg_compression_time = self.metrics.avg_compression_time_ms
        if avg_compression_time > 10:
            recommendations.append(
                "Average compression time exceeds 10ms - consider LZ4 for hot data"
            )

        # Generate optimization suggestions
        if memory_savings < 40:  # Below target
            optimizations['increase_compression'] = {
                'action': 'Lower thresholds and increase compression levels',
                'expected_improvement': '10-15% additional memory savings'
            }

        if avg_compression_time > 5:
            optimizations['improve_speed'] = {
                'action': 'Switch hot data to LZ4 algorithm',
                'expected_improvement': '50-70% faster compression'
            }

        return {
            'current_performance': {
                'memory_savings_percent': memory_savings,
                'compression_ratio': overall_ratio,
                'avg_compression_time_ms': avg_compression_time,
                'cost_savings_usd_per_month': self.metrics.cost_savings_usd_per_month,
                'target_achieved': memory_savings >= 40
            },
            'algorithm_usage': self.metrics.algorithm_usage,
            'cache_performance': {
                'hit_rate': (
                    self.cache_hit_stats['hits'] /
                    (self.cache_hit_stats['hits'] + self.cache_hit_stats['misses'])
                    if (self.cache_hit_stats['hits'] + self.cache_hit_stats['misses']) > 0
                    else 0.0
                ),
                'cached_compressions': len(self.compression_cache)
            },
            'recommendations': recommendations,
            'optimizations': optimizations
        }

    def get_compression_stats(self) -> Dict[str, Any]:
        """Get comprehensive compression statistics."""

        return {
            'metrics': {
                'total_objects': self.metrics.total_objects,
                'compressed_objects': self.metrics.compressed_objects,
                'compression_ratio': round(self.metrics.compression_ratio, 2),
                'memory_savings_percent': round(self.metrics.memory_savings_percent, 1),
                'memory_saved_mb': round(
                    (self.metrics.bytes_before_compression - self.metrics.bytes_after_compression) / (1024**2),
                    2
                ),
                'avg_compression_time_ms': round(self.metrics.avg_compression_time_ms, 2),
                'cost_savings_usd_per_month': round(self.metrics.cost_savings_usd_per_month, 2)
            },
            'algorithm_performance': self.metrics.algorithm_usage,
            'profiles_configured': len(self.profiles),
            'cache_stats': self.cache_hit_stats,
            'performance_targets': {
                'memory_reduction_target': 40,
                'compression_time_target_ms': 10,
                'cost_savings_target_percent': 30
            }
        }


# Global enhanced compression service
_enhanced_compression: Optional[EnhancedRedisCompression] = None


def get_enhanced_compression_service() -> EnhancedRedisCompression:
    """Get singleton enhanced compression service."""
    global _enhanced_compression

    if _enhanced_compression is None:
        _enhanced_compression = EnhancedRedisCompression()

    return _enhanced_compression


# Export main classes
__all__ = [
    "EnhancedRedisCompression",
    "CompressionAlgorithm",
    "CompressionProfile",
    "CompressionMetrics",
    "get_enhanced_compression_service"
]