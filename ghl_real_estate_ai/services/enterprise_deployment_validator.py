"""
Enterprise Deployment Validator

Comprehensive validation system for enterprise-optimized Customer Intelligence Platform.
Validates all optimization components work together to meet performance targets:
- 500+ concurrent users
- <50ms cached response times  
- 95%+ cache hit rates
- 1M+ data point processing
"""

import asyncio
import time
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from contextlib import asynccontextmanager

import redis
import psycopg2
import streamlit as st
from anthropic import AsyncAnthropic

from .advanced_cache_service import EnterpriseRedisCache
from .advanced_db_optimizer import DatabaseOptimizer
from .streamlit_performance_optimizer import StreamlitOptimizer
from .async_task_manager import EnterpriseTaskManager
from .enterprise_monitoring import EnterpriseMonitoring
from .load_testing_framework import LoadTestingFramework


@dataclass
class ValidationResult:
    """Validation result with performance metrics."""
    component: str
    passed: bool
    metrics: Dict[str, float]
    errors: List[str]
    recommendations: List[str]


class EnterpriseDeploymentValidator:
    """
    Comprehensive enterprise deployment validation.
    
    Validates all optimization components meet enterprise performance targets.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.results: List[ValidationResult] = []
        
        # Performance targets
        self.targets = {
            'max_response_time_ms': 50,
            'min_cache_hit_rate': 0.95,
            'min_concurrent_users': 500,
            'max_data_points': 1_000_000,
            'max_memory_usage_mb': 2048,
            'min_uptime_percentage': 99.9
        }
        
        # Component instances
        self.cache: Optional[EnterpriseRedisCache] = None
        self.db_optimizer: Optional[DatabaseOptimizer] = None
        self.streamlit_optimizer: Optional[StreamlitOptimizer] = None
        self.task_manager: Optional[EnterpriseTaskManager] = None
        self.monitoring: Optional[EnterpriseMonitoring] = None
        self.load_tester: Optional[LoadTestingFramework] = None

    async def initialize_components(self) -> bool:
        """Initialize all optimization components for validation."""
        try:
            self.logger.info("Initializing enterprise optimization components...")
            
            # Initialize Redis cache
            self.cache = EnterpriseRedisCache(
                host='localhost',
                port=6379,
                max_connections=50,
                retry_on_timeout=True
            )
            
            # Initialize database optimizer
            self.db_optimizer = DatabaseOptimizer(
                connection_string="postgresql://user:pass@localhost:5432/ghl_db"
            )
            
            # Initialize Streamlit optimizer
            self.streamlit_optimizer = StreamlitOptimizer()
            
            # Initialize task manager
            self.task_manager = EnterpriseTaskManager(
                min_workers=5,
                max_workers=20
            )
            
            # Initialize monitoring
            self.monitoring = EnterpriseMonitoring()
            
            # Initialize load tester
            self.load_tester = LoadTestingFramework(
                base_url="http://localhost:8000"
            )
            
            # Start monitoring
            await self.monitoring.start()
            
            self.logger.info("‚úÖ All components initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Component initialization failed: {e}")
            return False

    async def validate_redis_cache_performance(self) -> ValidationResult:
        """Validate Redis cache meets enterprise performance targets."""
        component = "Redis Cache"
        errors = []
        recommendations = []
        metrics = {}
        
        try:
            self.logger.info(f"Validating {component}...")
            
            # Test connection pool
            start_time = time.time()
            connections = []
            for i in range(50):  # Test max connections
                conn = await self.cache.get_connection()
                connections.append(conn)
            connection_time = (time.time() - start_time) * 1000
            
            # Return connections
            for conn in connections:
                await self.cache.return_connection(conn)
            
            metrics['connection_pool_time_ms'] = connection_time
            
            # Test cache operations
            test_data = {f"key_{i}": f"value_{i}" for i in range(1000)}
            
            # Batch set operation
            start_time = time.time()
            await self.cache.set_many(test_data)
            batch_set_time = (time.time() - start_time) * 1000
            metrics['batch_set_time_ms'] = batch_set_time
            
            # Batch get operation
            start_time = time.time()
            retrieved_data = await self.cache.get_many(list(test_data.keys()))
            batch_get_time = (time.time() - start_time) * 1000
            metrics['batch_get_time_ms'] = batch_get_time
            
            # Calculate cache hit rate
            hit_rate = len(retrieved_data) / len(test_data)
            metrics['cache_hit_rate'] = hit_rate
            
            # Test circuit breaker
            circuit_breaker_works = await self._test_circuit_breaker()
            metrics['circuit_breaker_functional'] = 1 if circuit_breaker_works else 0
            
            # Validate against targets
            passed = True
            
            if batch_get_time > self.targets['max_response_time_ms']:
                passed = False
                errors.append(f"Cache response time {batch_get_time:.1f}ms exceeds target {self.targets['max_response_time_ms']}ms")
                recommendations.append("Consider Redis cluster or memory optimization")
            
            if hit_rate < self.targets['min_cache_hit_rate']:
                passed = False
                errors.append(f"Cache hit rate {hit_rate:.3f} below target {self.targets['min_cache_hit_rate']}")
                recommendations.append("Review cache TTL settings and invalidation strategy")
            
            if connection_time > 100:  # 100ms max for connection pool
                passed = False
                errors.append(f"Connection pool initialization too slow: {connection_time:.1f}ms")
                recommendations.append("Optimize Redis connection pool configuration")
            
            if not circuit_breaker_works:
                passed = False
                errors.append("Circuit breaker not functioning properly")
                recommendations.append("Review circuit breaker configuration and failover logic")
            
            if passed:
                self.logger.info(f"‚úÖ {component} validation passed")
            else:
                self.logger.warning(f"‚ö†Ô∏è {component} validation failed")
            
        except Exception as e:
            self.logger.error(f"‚ùå {component} validation error: {e}")
            passed = False
            errors.append(f"Validation exception: {str(e)}")
            recommendations.append("Check Redis server status and configuration")
        
        return ValidationResult(
            component=component,
            passed=passed,
            metrics=metrics,
            errors=errors,
            recommendations=recommendations
        )

    async def validate_database_performance(self) -> ValidationResult:
        """Validate database optimizer meets enterprise performance targets."""
        component = "Database Optimizer"
        errors = []
        recommendations = []
        metrics = {}
        
        try:
            self.logger.info(f"Validating {component}...")
            
            # Test connection pool
            pool_metrics = await self.db_optimizer.get_pool_metrics()
            metrics.update(pool_metrics)
            
            # Test query performance
            test_query = "SELECT COUNT(*) FROM leads WHERE status = 'active'"
            
            # Execute query multiple times to test caching
            query_times = []
            for i in range(10):
                start_time = time.time()
                await self.db_optimizer.execute_query(test_query)
                query_time = (time.time() - start_time) * 1000
                query_times.append(query_time)
            
            avg_query_time = sum(query_times) / len(query_times)
            min_query_time = min(query_times)
            metrics['avg_query_time_ms'] = avg_query_time
            metrics['min_query_time_ms'] = min_query_time
            metrics['query_cache_improvement'] = (query_times[0] - min_query_time) / query_times[0]
            
            # Test connection scaling
            connections_active = pool_metrics.get('active_connections', 0)
            metrics['connection_utilization'] = connections_active / 40  # Max connections
            
            # Health check
            health_status = await self.db_optimizer.health_check()
            metrics['health_score'] = health_status.get('score', 0)
            
            # Validate against targets
            passed = True
            
            if avg_query_time > self.targets['max_response_time_ms']:
                passed = False
                errors.append(f"Average query time {avg_query_time:.1f}ms exceeds target")
                recommendations.append("Add database indexes and optimize slow queries")
            
            if metrics['connection_utilization'] > 0.8:
                passed = False
                errors.append("Connection pool utilization too high")
                recommendations.append("Increase connection pool size or optimize connection usage")
            
            if health_status.get('score', 0) < 0.9:
                passed = False
                errors.append("Database health score below acceptable threshold")
                recommendations.append("Review database configuration and resource allocation")
            
            if passed:
                self.logger.info(f"‚úÖ {component} validation passed")
            else:
                self.logger.warning(f"‚ö†Ô∏è {component} validation failed")
                
        except Exception as e:
            self.logger.error(f"‚ùå {component} validation error: {e}")
            passed = False
            errors.append(f"Validation exception: {str(e)}")
            recommendations.append("Check database server status and connection configuration")
        
        return ValidationResult(
            component=component,
            passed=passed,
            metrics=metrics,
            errors=errors,
            recommendations=recommendations
        )

    async def validate_streamlit_performance(self) -> ValidationResult:
        """Validate Streamlit optimizer meets enterprise performance targets."""
        component = "Streamlit Optimizer"
        errors = []
        recommendations = []
        metrics = {}
        
        try:
            self.logger.info(f"Validating {component}...")
            
            # Simulate large dataset loading
            large_dataset = [{"id": i, "value": f"data_{i}"} for i in range(10000)]
            
            # Test lazy loading
            start_time = time.time()
            lazy_loader = self.streamlit_optimizer.create_lazy_loader(large_dataset, page_size=100)
            first_page = next(lazy_loader)
            lazy_load_time = (time.time() - start_time) * 1000
            metrics['lazy_load_time_ms'] = lazy_load_time
            
            # Test caching
            cache_key = "test_dataset"
            start_time = time.time()
            self.streamlit_optimizer.cache_data(cache_key, large_dataset, ttl=300)
            cache_set_time = (time.time() - start_time) * 1000
            
            start_time = time.time()
            cached_data = self.streamlit_optimizer.get_cached_data(cache_key)
            cache_get_time = (time.time() - start_time) * 1000
            
            metrics['cache_set_time_ms'] = cache_set_time
            metrics['cache_get_time_ms'] = cache_get_time
            metrics['cache_hit'] = 1 if cached_data else 0
            
            # Test memory monitoring
            memory_stats = self.streamlit_optimizer.get_memory_stats()
            metrics.update(memory_stats)
            
            # Test session state optimization
            session_metrics = self.streamlit_optimizer.get_session_metrics()
            metrics.update(session_metrics)
            
            # Validate against targets
            passed = True
            
            if lazy_load_time > 100:  # 100ms max for lazy loading
                passed = False
                errors.append(f"Lazy loading too slow: {lazy_load_time:.1f}ms")
                recommendations.append("Optimize data chunking and page size")
            
            if cache_get_time > self.targets['max_response_time_ms']:
                passed = False
                errors.append(f"Cache retrieval too slow: {cache_get_time:.1f}ms")
                recommendations.append("Review caching strategy and data serialization")
            
            memory_usage_mb = memory_stats.get('memory_usage_mb', 0)
            if memory_usage_mb > self.targets['max_memory_usage_mb']:
                passed = False
                errors.append(f"Memory usage {memory_usage_mb}MB exceeds target")
                recommendations.append("Implement memory optimization and garbage collection")
            
            if not cached_data:
                passed = False
                errors.append("Caching mechanism not working")
                recommendations.append("Check cache configuration and storage backend")
            
            if passed:
                self.logger.info(f"‚úÖ {component} validation passed")
            else:
                self.logger.warning(f"‚ö†Ô∏è {component} validation failed")
                
        except Exception as e:
            self.logger.error(f"‚ùå {component} validation error: {e}")
            passed = False
            errors.append(f"Validation exception: {str(e)}")
            recommendations.append("Check Streamlit optimizer configuration")
        
        return ValidationResult(
            component=component,
            passed=passed,
            metrics=metrics,
            errors=errors,
            recommendations=recommendations
        )

    async def validate_async_task_performance(self) -> ValidationResult:
        """Validate async task manager meets enterprise performance targets."""
        component = "Async Task Manager"
        errors = []
        recommendations = []
        metrics = {}
        
        try:
            self.logger.info(f"Validating {component}...")
            
            # Test task submission and processing
            async def sample_task(task_id: int, duration: float = 0.1):
                await asyncio.sleep(duration)
                return f"Task {task_id} completed"
            
            # Submit multiple tasks
            start_time = time.time()
            task_futures = []
            for i in range(100):
                future = await self.task_manager.submit_task(
                    sample_task,
                    task_id=i,
                    duration=0.05,
                    priority=1 if i % 10 == 0 else 2  # Some high priority
                )
                task_futures.append(future)
            
            # Wait for completion
            results = await asyncio.gather(*task_futures)
            total_time = (time.time() - start_time) * 1000
            
            metrics['task_submission_time_ms'] = total_time
            metrics['tasks_completed'] = len(results)
            metrics['avg_task_time_ms'] = total_time / len(results)
            
            # Test worker scaling
            worker_metrics = await self.task_manager.get_worker_metrics()
            metrics.update(worker_metrics)
            
            # Test priority queue
            priority_metrics = await self.task_manager.get_queue_metrics()
            metrics.update(priority_metrics)
            
            # Test circuit breaker
            circuit_breaker_metrics = await self.task_manager.get_circuit_breaker_metrics()
            metrics.update(circuit_breaker_metrics)
            
            # Validate against targets
            passed = True
            
            if metrics['avg_task_time_ms'] > 100:  # 100ms max per task
                passed = False
                errors.append(f"Average task time too high: {metrics['avg_task_time_ms']:.1f}ms")
                recommendations.append("Optimize task processing and worker allocation")
            
            active_workers = worker_metrics.get('active_workers', 0)
            if active_workers < 5:
                passed = False
                errors.append("Insufficient worker scaling")
                recommendations.append("Review auto-scaling configuration")
            
            queue_depth = priority_metrics.get('queue_depth', 0)
            if queue_depth > 1000:
                passed = False
                errors.append("Task queue depth too high")
                recommendations.append("Increase worker capacity or optimize task processing")
            
            if passed:
                self.logger.info(f"‚úÖ {component} validation passed")
            else:
                self.logger.warning(f"‚ö†Ô∏è {component} validation failed")
                
        except Exception as e:
            self.logger.error(f"‚ùå {component} validation error: {e}")
            passed = False
            errors.append(f"Validation exception: {str(e)}")
            recommendations.append("Check task manager configuration and worker status")
        
        return ValidationResult(
            component=component,
            passed=passed,
            metrics=metrics,
            errors=errors,
            recommendations=recommendations
        )

    async def validate_monitoring_system(self) -> ValidationResult:
        """Validate enterprise monitoring meets performance targets."""
        component = "Enterprise Monitoring"
        errors = []
        recommendations = []
        metrics = {}
        
        try:
            self.logger.info(f"Validating {component}...")
            
            # Test metric collection
            test_metrics = {
                'response_time': 25.5,
                'cache_hit_rate': 0.98,
                'memory_usage': 1024,
                'concurrent_users': 450
            }
            
            start_time = time.time()
            await self.monitoring.record_metrics(test_metrics)
            metric_record_time = (time.time() - start_time) * 1000
            metrics['metric_record_time_ms'] = metric_record_time
            
            # Test alerting system
            start_time = time.time()
            alert_sent = await self.monitoring.send_alert(
                level='INFO',
                message='Validation test alert',
                component='validator'
            )
            alert_time = (time.time() - start_time) * 1000
            metrics['alert_time_ms'] = alert_time
            metrics['alert_sent'] = 1 if alert_sent else 0
            
            # Test health checks
            health_status = await self.monitoring.get_system_health()
            metrics['system_health_score'] = health_status.get('overall_score', 0)
            
            # Test anomaly detection
            anomaly_detected = await self.monitoring.detect_anomaly('response_time', 150.0)
            metrics['anomaly_detection_working'] = 1 if anomaly_detected else 0
            
            # Test SLA monitoring
            sla_status = await self.monitoring.get_sla_status()
            metrics['sla_compliance'] = sla_status.get('uptime_percentage', 0)
            
            # Validate against targets
            passed = True
            
            if metric_record_time > 50:  # 50ms max for metric recording
                passed = False
                errors.append(f"Metric recording too slow: {metric_record_time:.1f}ms")
                recommendations.append("Optimize metric storage and batching")
            
            if not alert_sent:
                passed = False
                errors.append("Alert system not functioning")
                recommendations.append("Check alert configuration and delivery channels")
            
            if health_status.get('overall_score', 0) < 0.9:
                passed = False
                errors.append("System health score below threshold")
                recommendations.append("Review system components and resolve health issues")
            
            if sla_status.get('uptime_percentage', 0) < self.targets['min_uptime_percentage']:
                passed = False
                errors.append("SLA compliance below target")
                recommendations.append("Improve system reliability and monitoring coverage")
            
            if passed:
                self.logger.info(f"‚úÖ {component} validation passed")
            else:
                self.logger.warning(f"‚ö†Ô∏è {component} validation failed")
                
        except Exception as e:
            self.logger.error(f"‚ùå {component} validation error: {e}")
            passed = False
            errors.append(f"Validation exception: {str(e)}")
            recommendations.append("Check monitoring system configuration")
        
        return ValidationResult(
            component=component,
            passed=passed,
            metrics=metrics,
            errors=errors,
            recommendations=recommendations
        )

    async def validate_load_testing_capability(self) -> ValidationResult:
        """Validate load testing framework meets enterprise targets."""
        component = "Load Testing Framework"
        errors = []
        recommendations = []
        metrics = {}
        
        try:
            self.logger.info(f"Validating {component}...")
            
            # Run a scaled-down load test for validation
            test_config = {
                'concurrent_users': 50,  # Scaled down for validation
                'duration_seconds': 30,
                'ramp_up_time': 10
            }
            
            start_time = time.time()
            test_result = await self.load_tester.run_load_test(**test_config)
            test_duration = time.time() - start_time
            
            metrics['test_duration_seconds'] = test_duration
            metrics['concurrent_users_achieved'] = test_result.get('peak_concurrent_users', 0)
            metrics['total_requests'] = test_result.get('total_requests', 0)
            metrics['avg_response_time_ms'] = test_result.get('avg_response_time', 0)
            metrics['error_rate'] = test_result.get('error_rate', 1.0)
            
            # Test monitoring integration
            monitoring_data = test_result.get('monitoring_data', {})
            metrics['monitoring_integration'] = 1 if monitoring_data else 0
            
            # Test report generation
            report_generated = await self.load_tester.generate_report(test_result)
            metrics['report_generation'] = 1 if report_generated else 0
            
            # Validate framework capability
            passed = True
            
            if metrics['concurrent_users_achieved'] < test_config['concurrent_users'] * 0.9:
                passed = False
                errors.append("Failed to achieve target concurrent users")
                recommendations.append("Check load generator capacity and network configuration")
            
            if metrics['avg_response_time_ms'] > 100:  # Relaxed for validation
                passed = False
                errors.append("Response times too high during load test")
                recommendations.append("Optimize application performance or test environment")
            
            if metrics['error_rate'] > 0.01:  # Max 1% error rate
                passed = False
                errors.append("Error rate too high during load test")
                recommendations.append("Investigate and fix application errors")
            
            if not monitoring_data:
                passed = False
                errors.append("Monitoring integration not working")
                recommendations.append("Verify monitoring system integration")
            
            if not report_generated:
                passed = False
                errors.append("Report generation failed")
                recommendations.append("Check report generation configuration and storage")
            
            if passed:
                self.logger.info(f"‚úÖ {component} validation passed")
            else:
                self.logger.warning(f"‚ö†Ô∏è {component} validation failed")
                
        except Exception as e:
            self.logger.error(f"‚ùå {component} validation error: {e}")
            passed = False
            errors.append(f"Validation exception: {str(e)}")
            recommendations.append("Check load testing framework configuration")
        
        return ValidationResult(
            component=component,
            passed=passed,
            metrics=metrics,
            errors=errors,
            recommendations=recommendations
        )

    async def run_full_validation(self) -> Tuple[bool, List[ValidationResult]]:
        """Run complete enterprise deployment validation."""
        self.logger.info("üöÄ Starting enterprise deployment validation...")
        
        # Initialize components
        if not await self.initialize_components():
            return False, []
        
        # Run all validations
        validation_tasks = [
            self.validate_redis_cache_performance(),
            self.validate_database_performance(),
            self.validate_streamlit_performance(),
            self.validate_async_task_performance(),
            self.validate_monitoring_system(),
            self.validate_load_testing_capability()
        ]
        
        self.results = await asyncio.gather(*validation_tasks)
        
        # Overall validation status
        all_passed = all(result.passed for result in self.results)
        
        # Generate summary
        self._generate_validation_summary()
        
        return all_passed, self.results

    def _generate_validation_summary(self):
        """Generate validation summary report."""
        passed_count = sum(1 for result in self.results if result.passed)
        total_count = len(self.results)
        
        self.logger.info(f"\n" + "="*60)
        self.logger.info(f"ENTERPRISE DEPLOYMENT VALIDATION SUMMARY")
        self.logger.info(f"="*60)
        self.logger.info(f"Overall Status: {'‚úÖ PASSED' if passed_count == total_count else '‚ùå FAILED'}")
        self.logger.info(f"Components Passed: {passed_count}/{total_count}")
        self.logger.info("")
        
        for result in self.results:
            status = "‚úÖ PASS" if result.passed else "‚ùå FAIL"
            self.logger.info(f"{result.component}: {status}")
            
            if result.errors:
                self.logger.info(f"  Errors: {len(result.errors)}")
                for error in result.errors[:3]:  # Show first 3 errors
                    self.logger.info(f"    - {error}")
            
            if result.recommendations:
                self.logger.info(f"  Recommendations: {len(result.recommendations)}")
                for rec in result.recommendations[:2]:  # Show first 2 recommendations
                    self.logger.info(f"    - {rec}")
            
            # Show key metrics
            key_metrics = ['response_time_ms', 'cache_hit_rate', 'concurrent_users', 'error_rate']
            shown_metrics = []
            for metric_key in key_metrics:
                for full_key, value in result.metrics.items():
                    if metric_key in full_key:
                        shown_metrics.append(f"{full_key}={value}")
                        break
            
            if shown_metrics:
                self.logger.info(f"  Key Metrics: {', '.join(shown_metrics[:3])}")
            
            self.logger.info("")
        
        self.logger.info("="*60)

    async def _test_circuit_breaker(self) -> bool:
        """Test circuit breaker functionality."""
        try:
            # This would test the circuit breaker by simulating failures
            # For validation, we'll just check if the circuit breaker exists
            return hasattr(self.cache, '_circuit_breaker')
        except Exception:
            return False

    async def cleanup(self):
        """Cleanup resources after validation."""
        try:
            if self.task_manager:
                await self.task_manager.stop()
            if self.monitoring:
                await self.monitoring.stop()
            if self.cache:
                await self.cache.close()
            if self.db_optimizer:
                await self.db_optimizer.close()
                
            self.logger.info("‚úÖ Validation cleanup completed")
        except Exception as e:
            self.logger.error(f"‚ùå Cleanup error: {e}")


# CLI interface for validation
async def main():
    """Run enterprise deployment validation from command line."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    validator = EnterpriseDeploymentValidator()
    
    try:
        all_passed, results = await validator.run_full_validation()
        
        if all_passed:
            print("\nüéâ ENTERPRISE DEPLOYMENT VALIDATION SUCCESSFUL!")
            print("All components meet enterprise performance targets.")
            exit(0)
        else:
            print("\n‚ùå ENTERPRISE DEPLOYMENT VALIDATION FAILED!")
            print("Some components require optimization before deployment.")
            exit(1)
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Validation interrupted by user")
        exit(1)
    except Exception as e:
        print(f"\n‚ùå Validation failed with exception: {e}")
        exit(1)
    finally:
        await validator.cleanup()


if __name__ == "__main__":
    asyncio.run(main())