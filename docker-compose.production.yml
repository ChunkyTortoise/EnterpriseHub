version: '3.8'

services:
  # Load Balancer (nginx)
  nginx:
    image: nginx:1.25-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - app-1
      - app-2
      - app-3
    restart: unless-stopped
    networks:
      - lead-intelligence

  # Application Instances
  app-1: &app-template
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
    environment:
      - INSTANCE_ID=app-1
      - REDIS_URL=redis://redis-cluster:6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - WORKER_CONCURRENCY=4
      - MAX_CONCURRENT_REQUESTS=100
    env_file:
      - .env.production
    restart: unless-stopped
    networks:
      - lead-intelligence
    depends_on:
      - redis-cluster
      - kafka
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.5"
        reservations:
          memory: 1G
          cpus: "0.5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  app-2:
    <<: *app-template
    environment:
      - INSTANCE_ID=app-2
      - REDIS_URL=redis://redis-cluster:6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - WORKER_CONCURRENCY=4
      - MAX_CONCURRENT_REQUESTS=100

  app-3:
    <<: *app-template
    environment:
      - INSTANCE_ID=app-3
      - REDIS_URL=redis://redis-cluster:6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - WORKER_CONCURRENCY=4
      - MAX_CONCURRENT_REQUESTS=100

  # Redis Cluster
  redis-cluster:
    image: redis/redis-stack:7.2.0-v6
    ports:
      - "6379:6379"
      - "8001:8001"  # RedisInsight
    volumes:
      - redis_data:/data
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: always
    networks:
      - lead-intelligence
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.0"

  # Kafka & Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    volumes:
      - zk_data:/var/lib/zookeeper/data
      - zk_logs:/var/lib/zookeeper/log
    restart: always
    networks:
      - lead-intelligence

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 104857600
      KAFKA_COMPRESSION_TYPE: snappy
    volumes:
      - kafka_data:/var/lib/kafka/data
    restart: always
    networks:
      - lead-intelligence
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.0"

  # PostgreSQL (for persistent data)
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: lead_intelligence
      POSTGRES_USER: lead_user
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
    secrets:
      - db_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db:/docker-entrypoint-initdb.d
    restart: always
    networks:
      - lead-intelligence
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "0.5"

  # Monitoring Stack
  prometheus:
    image: prom/prometheus:v2.45.0
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - lead-intelligence

  grafana:
    image: grafana/grafana:10.0.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD_FILE=/run/secrets/grafana_password
    secrets:
      - grafana_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    networks:
      - lead-intelligence

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:1.47
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    restart: unless-stopped
    networks:
      - lead-intelligence

  # Log aggregation
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    volumes:
      - es_data:/usr/share/elasticsearch/data
    restart: unless-stopped
    networks:
      - lead-intelligence

  logstash:
    image: docker.elastic.co/logstash/logstash:8.9.0
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    depends_on:
      - elasticsearch
    restart: unless-stopped
    networks:
      - lead-intelligence

  kibana:
    image: docker.elastic.co/kibana/kibana:8.9.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped
    networks:
      - lead-intelligence

secrets:
  db_password:
    file: ./secrets/db_password.txt
  grafana_password:
    file: ./secrets/grafana_password.txt

volumes:
  redis_data:
  postgres_data:
  kafka_data:
  zk_data:
  zk_logs:
  prometheus_data:
  grafana_data:
  es_data:

networks:
  lead-intelligence:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16